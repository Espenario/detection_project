{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dV66G0WGGuBj"
      },
      "source": [
        "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=500></p>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9LuFLCm3GuBl"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z7pPmypAGuBm"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Object detection</b></h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QLt9nwPoGuBn"
      },
      "source": [
        "### Руководитель проекта:\n",
        "* Юрий Яровиков (AIRI, МФТИ) | tg:@yu_rovikov"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IwjXeeaDGuBp"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Треки на проекте</b></h1>\n",
        "На этом проекте есть два возможных трека, из которых нужно выбрать один.\n",
        "\n",
        "* **Первый трек --- исследовательский**. На этом треке вам предстоит самостоятельно обучить и протестировать предобученную модель детекции. Основной упор делается на моделирование и обучение. Необходимо будет попробовать несколько моделей детекции, самостоятельно реализовать метрики.\n",
        "\n",
        "* **Второй трек --- продуктовый**. На этом треке вам не понадобится обучать свою модель детекции (хотя никто не запрещает вам это делать), но необходимо, во-первых, продумать **продуктовую составляющую проекта** (проблема людей, которая решается в данном проекте, целевая аудитория продукта, оптимальный способ внедрения модели), а также создать [MVP](https://ru.wikipedia.org/wiki/%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE_%D0%B6%D0%B8%D0%B7%D0%BD%D0%B5%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B4%D1%83%D0%BA%D1%82) , **внедрив модель в цифровой сервис**, который может быть реализован как Telegram-бот, Web-демо, Desktop-приложение.\n",
        "\n",
        "Вам необходимо выбрать основной сценарий, по которому вы пойдете, указав это при сдаче работы. При этом, никто не мешает вам совместить два трека, проведя и моделирование, и встраивание в демо. В этом случае мы рекомендуем пойти по **плану из второго трека**, а за моделирование будут ставиться бонусные баллы.\n",
        "\n",
        "Обратите внимание, что суммарный балл по проекту не может превышать 10. Максимальный балл можно получить на любом из двух треков."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "58WM1dAoU5cV"
      },
      "source": [
        "# Исследовательский трек\n",
        "На этом треке вам предстоит самостоятельно обучить и протестировать предобученную модель детекции. Основной упор делается на моделирование и обучение. Необходимо попробовать несколько моделей детекции и провести их объективное сравнение в соответствии с целевой метрикой проекта.\n",
        "\n",
        "## План работы\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_JHEakt_Bb6"
      },
      "source": [
        "### 1. Выбор фреймворка/библиотеки для использования детектора (1 балл)\n",
        "\n",
        "Чтобы освежить память о задаче детекции, можно посмотреть [занятия на продвинутом курсе](https://stepik.org/lesson/458312/step/1?unit=616130).\n",
        "\n",
        "В выборе фреймворка предоставляется свобода, лично я рекомендовал бы один из:\n",
        "- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n",
        "- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n",
        "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n",
        "- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
        "\n",
        "**Обратите внимание, что для получения полного балла по проекту необходимо обучить и сравнить как минимум две различные модели детекции (можно из одного фреймворка)!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l3mg1D_W4g-"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OkobRpd6W3An"
      },
      "source": [
        "### 2. Запуск детектора на случайных изображениях (1 балл)\n",
        "\n",
        "В этом пункте вам необходимо применить модель детектирования в выбранном выше репозитории (по сути проверить, что инференс в модели работает). Таким образом, вы убедитесь, что модель работает, и сможет переходить к обучению.\n",
        "\n",
        "> Результатом пункта явлется набор изображений, на которых модель успешно отработала и результат детекции виден и понятен.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISt6_q_FW5Jm"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "90z-NuSbGuB5"
      },
      "source": [
        "### 3. Выбор датасета (0 баллов)\n",
        "\n",
        "Вы можете выбрать любой датасет для детекции. Вот несколько идей:\n",
        "1. [Детекция игровых карт](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10): лежат в папке images вместе с разметкой;\n",
        "2. [Детекция фруктов](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection): скачать можно, нажав на кнопку Download;\n",
        "3. [Детекция одежды (Deep Fashion 2)](https://github.com/switchablenorms/DeepFashion2): стоит прочитать README на главной странице репозитория. Для получения датасета нужно запросить пароль у автора через гугл-форму. После скачивания распакуйте его с использованием пароля. Из файлов аннотаций нас будут интересовать только `bounding_box`, `category_name` и `category_id`;\n",
        "4. [Детекция лиц (Wider Face)](http://shuoyang1213.me/WIDERFACE/): большой датасет для детектирования лиц самых разных размеров. Скачать можно прямо по ссылкам на сайте;\n",
        "5. [Детекция лиц (Kaggle)](https://www.kaggle.com/dataturks/face-detection-in-images): в датасете достаточно мало данных, но можно попробовать, если датасеты выше показались неподходящими для Вас;\n",
        "6. Датасет из любого соревновани по детекции на Kaggle.\n",
        "\n",
        "При работе с датасетом вы неизбежно столкнетесь с работой с файлами и папками (директориями). Рекомендуется освежить в памяти работу с библиотеками `os`, `json`, `glob`. Может помочь [этот туториал](https://realpython.com/working-with-files-in-python/).\n",
        "\n",
        "> Результатом выполнения пункта явлется загруженный датасет, состоящий из изображений и разметки к ним (bounding box'ов всех объектов на каждом изображении)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LEL6VxDXZcY"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Yz0JAWGuB6"
      },
      "source": [
        "### 4. Предобработка данных (2 балла)\n",
        "\n",
        "Самый непростой этап в этом сценарии. Скачать данные $-$ лишь половина дела. Чтобы обучить нейросеть на этих данных, нужно написать генератор батчей. Однако если будем подавать изображения так, как они есть, то даже батч собрать не сможем -- нужно привести их к однмоу размеру. Далее нужно привести их к типу float, переместить на CUDA и поделить значения в пикселях на 255 (подробнее см. [занятие](https://www.youtube.com/watch?v=XSPYe4-y4HE)). Также нужно настроить аугментации и постобработку.\n",
        "\n",
        "То, как именно все это реализовать $-$ зависит от инструмента, выбранного в пункте 1. Например, в detectron2 в обучающих материалах описан формат данных для обучения. Возможно, нужно будет зайти в документацию и почитать более подробно, чтобы разобраться, какой именно нужен формат координат.\n",
        "\n",
        "НЕ нужно копировать все файлы с картинками и разметкой прямо на диске в их предобработанные версии. Хороший тон $-$ осуществлять всю эту обработку программно, \"на лету\". Поможет [туториал](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) по написанию своего датасета на PyTorch.\n",
        "\n",
        "> Результатом выполнения пункта явлется код, запуск которого ведет к подаче батчей правильного вида (разметка приведена к требуемому формату координат, изображения нужного типа, размера и поделены на 255 и т.д.) для обучения нейронной сети-детектора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpDRtOU4XdP-"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3kOKLO9FGuB7"
      },
      "source": [
        "### 5. Обучение моделей-детекторов (3 балла)\n",
        "\n",
        "Необходимо написать цикл обучения на PyTorch самостоятельно -- это основной критерий в этом пункте. Необходимо обучить обе выбранные модели."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2XAXboBIGuB7"
      },
      "source": [
        "> Результатом выполнения пункта явлется код, запуск которого ведет к обучению модели на выбранном датасете. При обучении **обязательно выводить числовые значения лосса на трейне и валидации**, крайне желательно использовать [`TensorBoard`](https://pytorch.org/docs/stable/tensorboard.html) для визуализации. Обязательно также сохранять модель после каждой N-ой эпохи, чтобы потом ее качество можно было проверить и веса были переиспользуемыми."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LyZ2CgtoGuB8"
      },
      "source": [
        "### 6. Измерение качества работы модели (метрики согласуются с руководителем и зависят от задачи) (2 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "agZ_BOjEGuB9"
      },
      "source": [
        "Под метриками понимаются функции/формулы, по которым оценивается качество модели-детектора. Обычно для измерения качества работы детектора используют поклассовые Precision, Recall, F1-меру и mean Average Precision (mAP). Подробнее про них можно послушать в [видеолекции 2018 года](https://www.youtube.com/watch?v=ewkSI2cuyoQ&list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3&index=24).\n",
        "\n",
        "**Необходимо самостоятельно реализовать требуемые метрики!**\n",
        "\n",
        "> Результат пункта --- реализованные функции метрик для задачи детектирования, позволяющие оценить качество работы модели на выборке, а также оценка обеих обученных моделей по данным метрикам на test. Необходимо сделать вывод о том, какая модель сработала лучше и оценить полученный результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9IEmKseX3mw"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mnvIv-rbGuB9"
      },
      "source": [
        "### 7. Поиск путей применения этой модели в бизнесе/реальных задачах/набросок встраивания в веб/мобильное демо (1 балл)\n",
        "\n",
        "В этом пункте нужно подумать, как эта модель может быть использована в дальнейшем. То есть, например, зачем нужно детектировать фрукты? Или одежду?\n",
        "\n",
        "> Результат пункта $-$ перечисленные кейсы использования модели (описанные **как можно подробнее**).\n",
        "\n",
        "**IMPORTANT NOTE:** Обычно этим вопросом все же задаются до начала какой-либо разработки. Но поскольку проект носит учебный/исследовательский характер, допустимо говорить об этом в конце"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w5YtFZ0X3Gf"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTEStsXGuBv"
      },
      "source": [
        "# Продуктовый трек"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DbRJ7T6QGuBw"
      },
      "source": [
        "На этом треке вам не понадобится обучать свою модель детекции (хотя никто не запрещает вам это делать), но необходимо, во-первых, продумать **продуктовую составляющую проекта** (проблема людей, которая решается в данном проекте, целевая аудитория продукта, оптимальный способ внедрения модели), а также создать [MVP](https://ru.wikipedia.org/wiki/%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE_%D0%B6%D0%B8%D0%B7%D0%BD%D0%B5%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B4%D1%83%D0%BA%D1%82) , **внедрив модель детекции в цифровой сервис**, который может быть реализован как Telegram-бот, Web-демо, Desktop-приложение.\n",
        "\n",
        "Ваша модель не обязательно должна содержать в себе лишь детекцию: например, существуют составные модели, которые осуществляют детекцию лиц на фотографии и определяют их настроение/возраст. Такие модели тоже можно и даже желательно использовать, если того требует проект. Единственное требование --- чтобы детекция присутствовала в качестве основной/вспомогательной задачи.\n",
        "\n",
        "Если у Вас есть опыт веб- или мобильной разработки, можете работать в рамках привычных Вам инструментов. Главное, чтобы в итоге они позволяил встроить в себя нейросетевой детектор, на вход которому будут поступать картинки.\n",
        "\n",
        "Изображения на вход демо могут поступать с веб-камеры, из файлов, по ссылке или с камеры мобильного телефона -- способ должен вытекать из предполагаемого сценария применения вашего продукта. Демо должно показывать, что детектор успешно отрабатывает на поданных изображениях и находит нужные объекты."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vp4s0VHLZsOI"
      },
      "source": [
        "## План работы"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fTi_YuevZxqg"
      },
      "source": [
        "### 1. Поиск проблемы и описание решения (2 балла)\n",
        "В этом пункте необходимо сформулировать проблему реального мира и продумать, как именно она будет решаться с помощью вашего продукта.\n",
        "\n",
        "#### Как должен быть устроен ваш продукт\n",
        "Здесь мы не будем подробно обсуждать, как создавать IT-продукты, которые будут пользоваться широким спросом и способны генерировать выручку. Но кратко опишем!\n",
        "\n",
        "1. **Ваш продукт должен решать существующую проблему**. Исследования показывают, что это основная причина провала стартапов --- решение не существующей проблемы. О том, как создать стартап, который решает реальную проблему пользователя, можно прочитать [здесь](https://stfalcon.com/ru/blog/post/startups-solving-user-problems). Также есть отличная книжка \"Спроси маму\", которую необходимо прочитать любому человеку, который создает свой продукт.\n",
        "\n",
        "2. **Ваш продукт должен иметь целевую аудиторию**. Этот пункт увязан с предыдущим. Если у продукта нет целевой аудитории, его никто не будет использовать.\n",
        "\n",
        "3. **Ваш продукт должен быть оформлен в сервис, подходящий для основного сценария использования продукта и целевой аудитории**. Предположим, например, что вы делаете цифровой сервис для распознавания языка жестов. Как может выглядеть такой продукт и в какой сервис он может быть внедрён? Например, если создать ТГ-бота, который будет детектировать и распознавать жест по фотографии, его довольно сложно будет использовать, потому что каждый жест в отдельности сфотографировать нельзя. Оптимальным решением в этом случае было бы мобильное приложение с потоковым детектированием жеста на видео и автоматическим добавлением субтитров. При этом именно такой продукт может быть слишком сложен в реализации. Тогда необходимо выбрать оформление сервиса, которое будет осмысленно с продуктовой точки зрения и которое вы при этом сможете реализовать.\n",
        "\n",
        "#### Как искать проблему\n",
        "Есть много способов найти важную и актуальную проблему. Некоторые советы перечислены в книге \"Спроси маму\". Несколько коротких советов можно найти [здесь](https://vc.ru/life/1735-startup-ideas).\n",
        "* Можно подумать о темах, которые близки лично вам/вашим знакомым. Если проект решает проблему даже узкой целевой аудитории, это не страшно.\n",
        "* Можно найти уже существующий проект и улучшить его, обозначив, в чем преимущество вашего решения перед конкурентами.\n",
        "* У человечества вообще много глобальных [проблем](https://www.un.org/sustainabledevelopment/ru/sustainable-development-goals/), над которыми борются различные мировые организации. Если ваш проект способен хоть в каком-то частном случае продвинуться к решению этих проблем, это уже будет отлично.\n",
        "* Для поиска идей можно использовать датасеты с kaggle.\n",
        "\n",
        "> Результат пункта -- подробное описание проблемы, которую вы решаете, целевая аудитория использования продукта, а также **подробное** описание сервиса, который предлагается создать. Допускается описать \"идеальный продукт\", а затем создать MVP, имеющий отклонения от оптимального варианта, сославшись на ограниченное время/ресурсы для выполнения проекта. Но тогда это необходимо отдельно упомянуть в этом пункте."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hnfqSran-TZO"
      },
      "source": [
        "# Определение заболеваний растений\n",
        "## Проблема\n",
        "Все мы любим, когда на нашем загородном участке есть множество красивых цыетов и других различных растений. В то же время, не далеко не все из нас профессиональные агрономы, а красивых растений хочется всем. Значит, необходимо как-то научиться определять, чем болеет растение без вызова соответствующего специалиста.\n",
        "## Описание сервиса\n",
        "> **Телеграм-бот**\n",
        ">\n",
        "> Сервис планируется реализовать как телеграм-бот, в который можно будет загружать изображения и получать в качестве ответа фотографию с выделенными областями заражения растения и информацию о виде болезни.\n",
        ">\n",
        "> **Модели**\n",
        "> > *Простой вариант*\n",
        "> >\n",
        "> > Найти какой-нибудь готовый и размеченный датасет растений и применить на нем модель (будет использоваться 2-стадийный детектор, не очень конечно, но для mvp норм).\n",
        "> >\n",
        "> > *Интересный вариант*\n",
        "> >\n",
        "> > Подумать над аугментацией данных, добавлением различных растений, протестировать различные доступные модели (хотя бы singleshot). \n",
        ">\n",
        "> **ЦА**\n",
        ">\n",
        "> Дачкники-садоводы-любители\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nFRfmcVF4YrX"
      },
      "source": [
        "### 2. Поиск обученной модели и датасета (1 балл)\n",
        "\n",
        "В этом пункте вам необходимо выбрать модель, которую вы встроите в ваш продукт, и датасет, на котором вы эту модель будете тестировать.\n",
        "\n",
        "* Если вы найдете готовую модель, которую можно применить для вашей задачи, можно просто взять её. В этом случае с датасетом можно особо не заморачиваться. Достаточно в этом пункте запустить ваш детектор на нескольких (5-7) изображениях, на которых модель будет в итоге применяться, и проверить, что модель на них хорошо работает.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx3Gncop-VLr"
      },
      "source": [
        "Подходящей обученной модели в открытом доступе не нашлось, поэтому будем обучать сами, используя готовый шаблон и датасет\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGG31_2V7zo7"
      },
      "source": [
        "### Бонус. 2.5. Обучение модели для вашей задачи (5 баллов)\n",
        "**За этот пункт ставятся бонусные баллы. Он не является обязательным.**\n",
        "\n",
        "Если готовой обученной модели вы не смогли найти, тогда необходимо будет обучить модель самостоятельно. В таком случае перед выполнением пункта 2 вам необходимо будет найти подходящий датасет и обучить любую модель детекции с помощью встроенных методов из фреймворков, описанных в первом сценарии:\n",
        "- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n",
        "- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n",
        "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n",
        "- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
        "\n",
        "После обучения модель нужно будет протестировать на real-world изображениях, на которых планируется использовать продукт."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: conda-script.py [-h] [-V] command ...\n",
            "conda-script.py: error: unrecognized arguments: --user https://download.pytorch.org/whl/cu113/torch_stable.html\n"
          ]
        }
      ],
      "source": [
        "! conda install --user torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LDVp3cSEwpcQ",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using distributed mode\n",
            "device cuda\n",
            "Checking Labels and images...\n",
            "Checking Labels and images...\n",
            "Creating data loaders\n",
            "Number of training samples: 4656\n",
            "Number of validation samples: 478\n",
            "\n",
            "Building model from scratch...\n",
            "=========================================================================================================\n",
            "Layer (type (var_name))                                 Output Shape              Param #\n",
            "=========================================================================================================\n",
            "FasterRCNN (FasterRCNN)                                 [0, 4]                    --\n",
            "├─GeneralizedRCNNTransform (transform)                  [4, 3, 800, 800]          --\n",
            "├─BackboneWithFPN (backbone)                            [4, 256, 13, 13]          --\n",
            "│    └─IntermediateLayerGetter (body)                   [4, 2048, 25, 25]         --\n",
            "│    │    └─Conv2d (conv1)                              [4, 64, 400, 400]         (9,408)\n",
            "│    │    └─FrozenBatchNorm2d (bn1)                     [4, 64, 400, 400]         --\n",
            "│    │    └─ReLU (relu)                                 [4, 64, 400, 400]         --\n",
            "│    │    └─MaxPool2d (maxpool)                         [4, 64, 200, 200]         --\n",
            "│    │    └─Sequential (layer1)                         [4, 256, 200, 200]        (212,992)\n",
            "│    │    └─Sequential (layer2)                         [4, 512, 100, 100]        1,212,416\n",
            "│    │    └─Sequential (layer3)                         [4, 1024, 50, 50]         7,077,888\n",
            "│    │    └─Sequential (layer4)                         [4, 2048, 25, 25]         14,942,208\n",
            "│    └─FeaturePyramidNetwork (fpn)                      [4, 256, 13, 13]          --\n",
            "│    │    └─ModuleList (inner_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (layer_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (inner_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (layer_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (inner_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (layer_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (inner_blocks)                   --                        (recursive)\n",
            "│    │    └─ModuleList (layer_blocks)                   --                        (recursive)\n",
            "│    │    └─LastLevelMaxPool (extra_blocks)             [4, 256, 200, 200]        --\n",
            "├─RegionProposalNetwork (rpn)                           [1000, 4]                 --\n",
            "│    └─RPNHead (head)                                   [4, 3, 200, 200]          --\n",
            "│    │    └─Conv2d (conv)                               [4, 256, 200, 200]        590,080\n",
            "│    │    └─Conv2d (cls_logits)                         [4, 3, 200, 200]          771\n",
            "│    │    └─Conv2d (bbox_pred)                          [4, 12, 200, 200]         3,084\n",
            "│    │    └─Conv2d (conv)                               [4, 256, 100, 100]        (recursive)\n",
            "│    │    └─Conv2d (cls_logits)                         [4, 3, 100, 100]          (recursive)\n",
            "│    │    └─Conv2d (bbox_pred)                          [4, 12, 100, 100]         (recursive)\n",
            "│    │    └─Conv2d (conv)                               [4, 256, 50, 50]          (recursive)\n",
            "│    │    └─Conv2d (cls_logits)                         [4, 3, 50, 50]            (recursive)\n",
            "│    │    └─Conv2d (bbox_pred)                          [4, 12, 50, 50]           (recursive)\n",
            "│    │    └─Conv2d (conv)                               [4, 256, 25, 25]          (recursive)\n",
            "│    │    └─Conv2d (cls_logits)                         [4, 3, 25, 25]            (recursive)\n",
            "│    │    └─Conv2d (bbox_pred)                          [4, 12, 25, 25]           (recursive)\n",
            "│    │    └─Conv2d (conv)                               [4, 256, 13, 13]          (recursive)\n",
            "│    │    └─Conv2d (cls_logits)                         [4, 3, 13, 13]            (recursive)\n",
            "│    │    └─Conv2d (bbox_pred)                          [4, 12, 13, 13]           (recursive)\n",
            "│    └─AnchorGenerator (anchor_generator)               [159882, 4]               --\n",
            "├─RoIHeads (roi_heads)                                  [0, 4]                    --\n",
            "│    └─MultiScaleRoIAlign (box_roi_pool)                [4000, 256, 7, 7]         --\n",
            "│    └─TwoMLPHead (box_head)                            [4000, 1024]              --"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: golovkin213171 (peppapig). Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.15.4\n",
            "wandb: Run data is saved locally in e:\\dls\\detection_project\\detection_stuff\\wandb\\run-20230705_100022-z1c45fo0\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run fasterrcnn_resnet50_fpn_v2_trainaug_30e\n",
            "wandb:  View project at https://wandb.ai/peppapig/detection_project\n",
            "wandb:  View run at https://wandb.ai/peppapig/detection_project/runs/z1c45fo0\n",
            "\n",
            "  0%|          | 0/4656 [00:00<?, ?it/s]\n",
            "100%|██████████| 4656/4656 [00:00<00:00, 68470.75it/s]\n",
            "\n",
            "  0%|          | 0/478 [00:00<?, ?it/s]\n",
            "100%|██████████| 478/478 [00:00<00:00, 239017.32it/s]\n",
            "c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Epoch: [0]  [   0/1164]  eta: 4:03:01  lr: 0.000002  loss: 4.6216 (4.6216)  loss_classifier: 3.4770 (3.4770)  loss_box_reg: 0.2442 (0.2442)  loss_objectness: 0.6900 (0.6900)  loss_rpn_box_reg: 0.2104 (0.2104)  time: 12.5268  data: 11.1146  max mem: 3009\n",
            "Epoch: [0]  [ 100/1164]  eta: 0:09:19  lr: 0.000102  loss: 1.9715 (3.4869)  loss_classifier: 0.8151 (2.3984)  loss_box_reg: 0.2841 (0.2191)  loss_objectness: 0.6341 (0.6743)  loss_rpn_box_reg: 0.1589 (0.1951)  time: 0.4070  data: 0.0031  max mem: 3277\n",
            "Epoch: [0]  [ 200/1164]  eta: 0:07:30  lr: 0.000202  loss: 1.9414 (2.7090)  loss_classifier: 0.8839 (1.6260)  loss_box_reg: 0.5472 (0.3501)  loss_objectness: 0.3268 (0.5457)  loss_rpn_box_reg: 0.1121 (0.1871)  time: 0.4098  data: 0.0030  max mem: 3277\n",
            "Epoch: [0]  [ 300/1164]  eta: 0:06:27  lr: 0.000302  loss: 1.8097 (2.4302)  loss_classifier: 0.8414 (1.3638)  loss_box_reg: 0.5468 (0.4075)  loss_objectness: 0.3261 (0.4789)  loss_rpn_box_reg: 0.1120 (0.1800)  time: 0.4092  data: 0.0031  max mem: 3277\n",
            "Epoch: [0]  [ 400/1164]  eta: 0:05:35  lr: 0.000402  loss: 2.1232 (2.3345)  loss_classifier: 0.9561 (1.2592)  loss_box_reg: 0.6639 (0.4666)  loss_objectness: 0.2497 (0.4330)  loss_rpn_box_reg: 0.1437 (0.1756)  time: 0.4114  data: 0.0035  max mem: 3277\n",
            "Epoch: [0]  [ 500/1164]  eta: 0:04:48  lr: 0.000501  loss: 2.1484 (2.3112)  loss_classifier: 1.0284 (1.2137)  loss_box_reg: 0.8283 (0.5278)  loss_objectness: 0.1890 (0.3947)  loss_rpn_box_reg: 0.1188 (0.1750)  time: 0.4111  data: 0.0033  max mem: 3277\n",
            "Epoch: [0]  [ 600/1164]  eta: 0:04:02  lr: 0.000601  loss: 2.0069 (2.2849)  loss_classifier: 0.9705 (1.1769)  loss_box_reg: 0.7936 (0.5730)  loss_objectness: 0.1865 (0.3648)  loss_rpn_box_reg: 0.0982 (0.1701)  time: 0.4140  data: 0.0032  max mem: 3277\n",
            "Epoch: [0]  [ 700/1164]  eta: 0:03:18  lr: 0.000701  loss: 1.9732 (2.2546)  loss_classifier: 0.8775 (1.1430)  loss_box_reg: 0.7614 (0.6038)  loss_objectness: 0.1615 (0.3404)  loss_rpn_box_reg: 0.0920 (0.1674)  time: 0.4119  data: 0.0034  max mem: 3277\n",
            "Epoch: [0]  [ 800/1164]  eta: 0:02:34  lr: 0.000801  loss: 1.9088 (2.2190)  loss_classifier: 0.8705 (1.1107)  loss_box_reg: 0.7662 (0.6235)  loss_objectness: 0.1882 (0.3198)  loss_rpn_box_reg: 0.1275 (0.1650)  time: 0.4097  data: 0.0033  max mem: 3277\n",
            "Epoch: [0]  [ 900/1164]  eta: 0:01:51  lr: 0.000901  loss: 1.8457 (2.1849)  loss_classifier: 0.8182 (1.0821)  loss_box_reg: 0.7217 (0.6373)  loss_objectness: 0.1417 (0.3024)  loss_rpn_box_reg: 0.1227 (0.1631)  time: 0.4112  data: 0.0035  max mem: 3277\n",
            "Epoch: [0]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 1.7167 (2.1430)  loss_classifier: 0.7523 (1.0529)  loss_box_reg: 0.7144 (0.6450)  loss_objectness: 0.1217 (0.2863)  loss_rpn_box_reg: 0.0990 (0.1588)  time: 0.4096  data: 0.0034  max mem: 3277\n",
            "Epoch: [0]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 1.7691 (2.1091)  loss_classifier: 0.7401 (1.0291)  loss_box_reg: 0.6888 (0.6506)  loss_objectness: 0.1438 (0.2730)  loss_rpn_box_reg: 0.0948 (0.1565)  time: 0.4111  data: 0.0031  max mem: 3277\n",
            "Epoch: [0]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 1.7029 (2.0881)  loss_classifier: 0.7826 (1.0157)  loss_box_reg: 0.6969 (0.6526)  loss_objectness: 0.1323 (0.2651)  loss_rpn_box_reg: 0.0993 (0.1547)  time: 0.4074  data: 0.0036  max mem: 3277\n",
            "Epoch: [0] Total time: 0:08:10 (0.4216 s / it)\n",
            "Test:  [  0/120]  eta: 0:22:18  model_time: 0.5200 (0.5200)  evaluator_time: 0.0120 (0.0120)  time: 11.1505  data: 10.5545  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1875 (0.1919)  evaluator_time: 0.0140 (0.0146)  time: 0.2159  data: 0.0032  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1873 (0.1906)  evaluator_time: 0.0140 (0.0154)  time: 0.2150  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:37 (0.3132 s / it)\n",
            "Epoch: [1]  [   0/1164]  eta: 4:27:13  lr: 0.001000  loss: 1.8115 (1.8115)  loss_classifier: 0.8765 (0.8765)  loss_box_reg: 0.6922 (0.6922)  loss_objectness: 0.1295 (0.1295)  loss_rpn_box_reg: 0.1133 (0.1133)  time: 13.7742  data: 12.9362  max mem: 3277\n",
            "Epoch: [1]  [ 100/1164]  eta: 0:09:37  lr: 0.001000  loss: 1.6007 (1.6449)  loss_classifier: 0.7287 (0.7433)  loss_box_reg: 0.6692 (0.6795)  loss_objectness: 0.0965 (0.1120)  loss_rpn_box_reg: 0.0762 (0.1101)  time: 0.4081  data: 0.0032  max mem: 3277\n",
            "Epoch: [1]  [ 200/1164]  eta: 0:07:39  lr: 0.001000  loss: 1.5756 (1.6214)  loss_classifier: 0.6695 (0.7263)  loss_box_reg: 0.6685 (0.6696)  loss_objectness: 0.0959 (0.1115)  loss_rpn_box_reg: 0.0969 (0.1140)  time: 0.4116  data: 0.0038  max mem: 3277\n",
            "Epoch: [1]  [ 300/1164]  eta: 0:06:33  lr: 0.001000  loss: 1.5585 (1.6059)  loss_classifier: 0.6669 (0.7181)  loss_box_reg: 0.6747 (0.6654)  loss_objectness: 0.1159 (0.1108)  loss_rpn_box_reg: 0.1010 (0.1116)  time: 0.4154  data: 0.0040  max mem: 3277\n",
            "Epoch: [1]  [ 400/1164]  eta: 0:05:40  lr: 0.001000  loss: 1.4830 (1.5855)  loss_classifier: 0.6330 (0.7093)  loss_box_reg: 0.6164 (0.6558)  loss_objectness: 0.1071 (0.1083)  loss_rpn_box_reg: 0.0702 (0.1121)  time: 0.4113  data: 0.0039  max mem: 3277\n",
            "Epoch: [1]  [ 500/1164]  eta: 0:04:51  lr: 0.001000  loss: 1.4298 (1.5683)  loss_classifier: 0.6447 (0.7000)  loss_box_reg: 0.6260 (0.6494)  loss_objectness: 0.0830 (0.1073)  loss_rpn_box_reg: 0.0710 (0.1116)  time: 0.4151  data: 0.0039  max mem: 3277\n",
            "Epoch: [1]  [ 600/1164]  eta: 0:04:05  lr: 0.001000  loss: 1.4685 (1.5568)  loss_classifier: 0.6706 (0.6940)  loss_box_reg: 0.6149 (0.6441)  loss_objectness: 0.0906 (0.1062)  loss_rpn_box_reg: 0.0733 (0.1124)  time: 0.4122  data: 0.0038  max mem: 3277\n",
            "Epoch: [1]  [ 700/1164]  eta: 0:03:20  lr: 0.001000  loss: 1.3669 (1.5389)  loss_classifier: 0.6156 (0.6852)  loss_box_reg: 0.5826 (0.6379)  loss_objectness: 0.0825 (0.1046)  loss_rpn_box_reg: 0.0757 (0.1112)  time: 0.4159  data: 0.0042  max mem: 3277\n",
            "Epoch: [1]  [ 800/1164]  eta: 0:02:36  lr: 0.001000  loss: 1.2997 (1.5235)  loss_classifier: 0.5478 (0.6760)  loss_box_reg: 0.5942 (0.6325)  loss_objectness: 0.0744 (0.1030)  loss_rpn_box_reg: 0.0827 (0.1120)  time: 0.4122  data: 0.0035  max mem: 3277\n",
            "Epoch: [1]  [ 900/1164]  eta: 0:01:52  lr: 0.001000  loss: 1.3599 (1.5098)  loss_classifier: 0.6011 (0.6698)  loss_box_reg: 0.5702 (0.6277)  loss_objectness: 0.0750 (0.1019)  loss_rpn_box_reg: 0.0668 (0.1104)  time: 0.4133  data: 0.0036  max mem: 3277\n",
            "Epoch: [1]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 1.3998 (1.4964)  loss_classifier: 0.5886 (0.6628)  loss_box_reg: 0.5649 (0.6221)  loss_objectness: 0.1012 (0.1017)  loss_rpn_box_reg: 0.0994 (0.1099)  time: 0.4145  data: 0.0041  max mem: 3277\n",
            "Epoch: [1]  [1100/1164]  eta: 0:00:27  lr: 0.001000  loss: 1.3711 (1.4816)  loss_classifier: 0.5561 (0.6558)  loss_box_reg: 0.5698 (0.6165)  loss_objectness: 0.0892 (0.1002)  loss_rpn_box_reg: 0.0971 (0.1092)  time: 0.4138  data: 0.0038  max mem: 3277\n",
            "Epoch: [1]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 1.3622 (1.4753)  loss_classifier: 0.5949 (0.6525)  loss_box_reg: 0.5748 (0.6137)  loss_objectness: 0.0863 (0.0999)  loss_rpn_box_reg: 0.0972 (0.1092)  time: 0.4108  data: 0.0038  max mem: 3277\n",
            "Epoch: [1] Total time: 0:08:14 (0.4251 s / it)\n",
            "Test:  [  0/120]  eta: 0:22:03  model_time: 0.5053 (0.5053)  evaluator_time: 0.0100 (0.0100)  time: 11.0266  data: 10.4453  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1850 (0.1890)  evaluator_time: 0.0140 (0.0137)  time: 0.2127  data: 0.0032  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1860 (0.1877)  evaluator_time: 0.0130 (0.0144)  time: 0.2105  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:37 (0.3089 s / it)\n",
            "Epoch: [2]  [   0/1164]  eta: 3:51:21  lr: 0.001000  loss: 1.5128 (1.5128)  loss_classifier: 0.5169 (0.5169)  loss_box_reg: 0.5879 (0.5879)  loss_objectness: 0.1046 (0.1046)  loss_rpn_box_reg: 0.3034 (0.3034)  time: 11.9255  data: 11.2275  max mem: 3277\n",
            "Epoch: [2]  [ 100/1164]  eta: 0:09:17  lr: 0.001000  loss: 1.3669 (1.3063)  loss_classifier: 0.5494 (0.5626)  loss_box_reg: 0.5574 (0.5553)  loss_objectness: 0.0889 (0.0860)  loss_rpn_box_reg: 0.1440 (0.1025)  time: 0.4111  data: 0.0033  max mem: 3277\n",
            "Epoch: [2]  [ 200/1164]  eta: 0:07:31  lr: 0.001000  loss: 1.2273 (1.2990)  loss_classifier: 0.5369 (0.5636)  loss_box_reg: 0.5389 (0.5526)  loss_objectness: 0.0698 (0.0828)  loss_rpn_box_reg: 0.0582 (0.1000)  time: 0.4120  data: 0.0034  max mem: 3277\n",
            "Epoch: [2]  [ 300/1164]  eta: 0:06:28  lr: 0.001000  loss: 1.2231 (1.2991)  loss_classifier: 0.5417 (0.5598)  loss_box_reg: 0.5379 (0.5473)  loss_objectness: 0.0753 (0.0854)  loss_rpn_box_reg: 0.0804 (0.1066)  time: 0.4137  data: 0.0033  max mem: 3277\n",
            "Epoch: [2]  [ 400/1164]  eta: 0:05:36  lr: 0.001000  loss: 1.2246 (1.2937)  loss_classifier: 0.5439 (0.5569)  loss_box_reg: 0.5279 (0.5453)  loss_objectness: 0.0917 (0.0853)  loss_rpn_box_reg: 0.0726 (0.1062)  time: 0.4129  data: 0.0035  max mem: 3277\n",
            "Epoch: [2]  [ 500/1164]  eta: 0:04:48  lr: 0.001000  loss: 1.2322 (1.2829)  loss_classifier: 0.5561 (0.5531)  loss_box_reg: 0.5250 (0.5400)  loss_objectness: 0.0765 (0.0841)  loss_rpn_box_reg: 0.0735 (0.1057)  time: 0.4116  data: 0.0033  max mem: 3277\n",
            "Epoch: [2]  [ 600/1164]  eta: 0:04:02  lr: 0.001000  loss: 1.2246 (1.2747)  loss_classifier: 0.5188 (0.5487)  loss_box_reg: 0.5210 (0.5378)  loss_objectness: 0.0720 (0.0830)  loss_rpn_box_reg: 0.0721 (0.1052)  time: 0.4113  data: 0.0032  max mem: 3277\n",
            "Epoch: [2]  [ 700/1164]  eta: 0:03:18  lr: 0.001000  loss: 1.0945 (1.2660)  loss_classifier: 0.4899 (0.5456)  loss_box_reg: 0.4923 (0.5349)  loss_objectness: 0.0672 (0.0819)  loss_rpn_box_reg: 0.0636 (0.1036)  time: 0.4112  data: 0.0033  max mem: 3277\n",
            "Epoch: [2]  [ 800/1164]  eta: 0:02:34  lr: 0.001000  loss: 1.1328 (1.2556)  loss_classifier: 0.4890 (0.5409)  loss_box_reg: 0.4889 (0.5308)  loss_objectness: 0.0632 (0.0810)  loss_rpn_box_reg: 0.0495 (0.1028)  time: 0.4121  data: 0.0035  max mem: 3277\n",
            "Epoch: [2]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 1.1407 (1.2458)  loss_classifier: 0.4810 (0.5361)  loss_box_reg: 0.5041 (0.5265)  loss_objectness: 0.0713 (0.0804)  loss_rpn_box_reg: 0.0663 (0.1029)  time: 0.4099  data: 0.0033  max mem: 3277\n",
            "Epoch: [2]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 1.1985 (1.2433)  loss_classifier: 0.4980 (0.5344)  loss_box_reg: 0.5162 (0.5249)  loss_objectness: 0.0851 (0.0801)  loss_rpn_box_reg: 0.0737 (0.1039)  time: 0.4112  data: 0.0034  max mem: 3277\n",
            "Epoch: [2]  [1100/1164]  eta: 0:00:27  lr: 0.001000  loss: 1.0666 (1.2358)  loss_classifier: 0.4295 (0.5302)  loss_box_reg: 0.4857 (0.5226)  loss_objectness: 0.0525 (0.0796)  loss_rpn_box_reg: 0.0629 (0.1033)  time: 0.4111  data: 0.0033  max mem: 3277\n",
            "Epoch: [2]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 1.2388 (1.2310)  loss_classifier: 0.4938 (0.5283)  loss_box_reg: 0.5203 (0.5211)  loss_objectness: 0.0675 (0.0790)  loss_rpn_box_reg: 0.0886 (0.1026)  time: 0.4074  data: 0.0035  max mem: 3277\n",
            "Epoch: [2] Total time: 0:08:11 (0.4219 s / it)\n",
            "Test:  [  0/120]  eta: 0:22:19  model_time: 0.4110 (0.4110)  evaluator_time: 0.0100 (0.0100)  time: 11.1630  data: 10.6660  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1860 (0.1884)  evaluator_time: 0.0150 (0.0145)  time: 0.2139  data: 0.0030  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1870 (0.1874)  evaluator_time: 0.0120 (0.0143)  time: 0.2071  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:37 (0.3098 s / it)\n",
            "Epoch: [3]  [   0/1164]  eta: 3:46:03  lr: 0.001000  loss: 0.8567 (0.8567)  loss_classifier: 0.4070 (0.4070)  loss_box_reg: 0.4014 (0.4014)  loss_objectness: 0.0316 (0.0316)  loss_rpn_box_reg: 0.0166 (0.0166)  time: 11.6529  data: 10.9309  max mem: 3277\n",
            "Epoch: [3]  [ 100/1164]  eta: 0:09:15  lr: 0.001000  loss: 1.0993 (1.1597)  loss_classifier: 0.4824 (0.4892)  loss_box_reg: 0.4894 (0.4976)  loss_objectness: 0.0759 (0.0705)  loss_rpn_box_reg: 0.0898 (0.1024)  time: 0.4113  data: 0.0034  max mem: 3277\n",
            "Epoch: [3]  [ 200/1164]  eta: 0:07:30  lr: 0.001000  loss: 1.0640 (1.1530)  loss_classifier: 0.4596 (0.4852)  loss_box_reg: 0.4509 (0.4923)  loss_objectness: 0.0573 (0.0720)  loss_rpn_box_reg: 0.0645 (0.1034)  time: 0.4122  data: 0.0033  max mem: 3277\n",
            "Epoch: [3]  [ 300/1164]  eta: 0:06:28  lr: 0.001000  loss: 1.1227 (1.1487)  loss_classifier: 0.4681 (0.4817)  loss_box_reg: 0.4817 (0.4903)  loss_objectness: 0.0612 (0.0715)  loss_rpn_box_reg: 0.0770 (0.1052)  time: 0.4147  data: 0.0038  max mem: 3277\n",
            "Epoch: [3]  [ 400/1164]  eta: 0:05:35  lr: 0.001000  loss: 1.1215 (1.1405)  loss_classifier: 0.4596 (0.4792)  loss_box_reg: 0.5073 (0.4885)  loss_objectness: 0.0686 (0.0712)  loss_rpn_box_reg: 0.0717 (0.1017)  time: 0.4112  data: 0.0033  max mem: 3277\n",
            "Epoch: [3]  [ 500/1164]  eta: 0:04:48  lr: 0.001000  loss: 1.0379 (1.1268)  loss_classifier: 0.4298 (0.4741)  loss_box_reg: 0.4479 (0.4835)  loss_objectness: 0.0630 (0.0702)  loss_rpn_box_reg: 0.0457 (0.0990)  time: 0.4130  data: 0.0032  max mem: 3277\n",
            "Epoch: [3]  [ 600/1164]  eta: 0:04:02  lr: 0.001000  loss: 1.0528 (1.1221)  loss_classifier: 0.4292 (0.4695)  loss_box_reg: 0.4754 (0.4816)  loss_objectness: 0.0611 (0.0707)  loss_rpn_box_reg: 0.0728 (0.1003)  time: 0.4152  data: 0.0032  max mem: 3277\n",
            "Epoch: [3]  [ 700/1164]  eta: 0:03:18  lr: 0.001000  loss: 1.0519 (1.1126)  loss_classifier: 0.4514 (0.4663)  loss_box_reg: 0.4591 (0.4790)  loss_objectness: 0.0644 (0.0699)  loss_rpn_box_reg: 0.0702 (0.0974)  time: 0.4117  data: 0.0037  max mem: 3277\n",
            "Epoch: [3]  [ 800/1164]  eta: 0:02:35  lr: 0.001000  loss: 1.0824 (1.1047)  loss_classifier: 0.4651 (0.4623)  loss_box_reg: 0.4760 (0.4765)  loss_objectness: 0.0642 (0.0691)  loss_rpn_box_reg: 0.0823 (0.0967)  time: 0.4125  data: 0.0031  max mem: 3277\n",
            "Epoch: [3]  [ 900/1164]  eta: 0:01:52  lr: 0.001000  loss: 1.1144 (1.0998)  loss_classifier: 0.4233 (0.4604)  loss_box_reg: 0.4725 (0.4749)  loss_objectness: 0.0674 (0.0684)  loss_rpn_box_reg: 0.1062 (0.0961)  time: 0.4120  data: 0.0036  max mem: 3277\n",
            "Epoch: [3]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 1.0024 (1.0946)  loss_classifier: 0.4245 (0.4587)  loss_box_reg: 0.4423 (0.4725)  loss_objectness: 0.0582 (0.0681)  loss_rpn_box_reg: 0.0680 (0.0952)  time: 0.4127  data: 0.0033  max mem: 3277\n",
            "Epoch: [3]  [1100/1164]  eta: 0:00:27  lr: 0.001000  loss: 1.0994 (1.0900)  loss_classifier: 0.4376 (0.4560)  loss_box_reg: 0.4709 (0.4714)  loss_objectness: 0.0601 (0.0677)  loss_rpn_box_reg: 0.0705 (0.0949)  time: 0.4137  data: 0.0038  max mem: 3277\n",
            "Epoch: [3]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.9799 (1.0854)  loss_classifier: 0.3998 (0.4531)  loss_box_reg: 0.4295 (0.4702)  loss_objectness: 0.0595 (0.0674)  loss_rpn_box_reg: 0.0647 (0.0947)  time: 0.4079  data: 0.0032  max mem: 3277\n",
            "Epoch: [3] Total time: 0:08:11 (0.4223 s / it)\n",
            "Test:  [  0/120]  eta: 0:21:56  model_time: 0.3710 (0.3710)  evaluator_time: 0.0080 (0.0080)  time: 10.9715  data: 10.5225  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1840 (0.1861)  evaluator_time: 0.0110 (0.0124)  time: 0.2092  data: 0.0031  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1860 (0.1853)  evaluator_time: 0.0110 (0.0123)  time: 0.2042  data: 0.0029  max mem: 3277\n",
            "Test: Total time: 0:00:36 (0.3045 s / it)\n",
            "Epoch: [4]  [   0/1164]  eta: 3:42:38  lr: 0.001000  loss: 1.1428 (1.1428)  loss_classifier: 0.4367 (0.4367)  loss_box_reg: 0.5018 (0.5018)  loss_objectness: 0.0882 (0.0882)  loss_rpn_box_reg: 0.1163 (0.1163)  time: 11.4766  data: 10.7666  max mem: 3277\n",
            "Epoch: [4]  [ 100/1164]  eta: 0:09:13  lr: 0.001000  loss: 1.0058 (1.0031)  loss_classifier: 0.3964 (0.4043)  loss_box_reg: 0.4422 (0.4513)  loss_objectness: 0.0585 (0.0650)  loss_rpn_box_reg: 0.0641 (0.0825)  time: 0.4109  data: 0.0033  max mem: 3277\n",
            "Epoch: [4]  [ 200/1164]  eta: 0:07:29  lr: 0.001000  loss: 1.0277 (1.0103)  loss_classifier: 0.4101 (0.4049)  loss_box_reg: 0.4570 (0.4458)  loss_objectness: 0.0623 (0.0657)  loss_rpn_box_reg: 0.0833 (0.0939)  time: 0.4140  data: 0.0034  max mem: 3277\n",
            "Epoch: [4]  [ 300/1164]  eta: 0:06:28  lr: 0.001000  loss: 0.9482 (1.0127)  loss_classifier: 0.3909 (0.4056)  loss_box_reg: 0.4335 (0.4436)  loss_objectness: 0.0772 (0.0662)  loss_rpn_box_reg: 0.0669 (0.0972)  time: 0.4155  data: 0.0037  max mem: 3277\n",
            "Epoch: [4]  [ 400/1164]  eta: 0:05:36  lr: 0.001000  loss: 1.0505 (1.0163)  loss_classifier: 0.4534 (0.4095)  loss_box_reg: 0.4413 (0.4431)  loss_objectness: 0.0727 (0.0666)  loss_rpn_box_reg: 0.0639 (0.0971)  time: 0.4105  data: 0.0033  max mem: 3277\n",
            "Epoch: [4]  [ 500/1164]  eta: 0:04:48  lr: 0.001000  loss: 0.9960 (1.0151)  loss_classifier: 0.3758 (0.4099)  loss_box_reg: 0.4206 (0.4428)  loss_objectness: 0.0512 (0.0660)  loss_rpn_box_reg: 0.0679 (0.0964)  time: 0.4128  data: 0.0032  max mem: 3277\n",
            "Epoch: [4]  [ 600/1164]  eta: 0:04:02  lr: 0.001000  loss: 0.8389 (1.0003)  loss_classifier: 0.3456 (0.4059)  loss_box_reg: 0.4117 (0.4390)  loss_objectness: 0.0444 (0.0639)  loss_rpn_box_reg: 0.0353 (0.0914)  time: 0.4099  data: 0.0035  max mem: 3277\n",
            "Epoch: [4]  [ 700/1164]  eta: 0:03:18  lr: 0.001000  loss: 0.8810 (0.9936)  loss_classifier: 0.3527 (0.4036)  loss_box_reg: 0.4099 (0.4373)  loss_objectness: 0.0507 (0.0629)  loss_rpn_box_reg: 0.0523 (0.0897)  time: 0.4172  data: 0.0037  max mem: 3277\n",
            "Epoch: [4]  [ 800/1164]  eta: 0:02:35  lr: 0.001000  loss: 1.0220 (0.9923)  loss_classifier: 0.3683 (0.4033)  loss_box_reg: 0.4356 (0.4364)  loss_objectness: 0.0577 (0.0624)  loss_rpn_box_reg: 0.0898 (0.0902)  time: 0.4090  data: 0.0032  max mem: 3277\n",
            "Epoch: [4]  [ 900/1164]  eta: 0:01:52  lr: 0.001000  loss: 0.8660 (0.9880)  loss_classifier: 0.3603 (0.4012)  loss_box_reg: 0.4152 (0.4346)  loss_objectness: 0.0414 (0.0621)  loss_rpn_box_reg: 0.0415 (0.0902)  time: 0.4087  data: 0.0033  max mem: 3277\n",
            "Epoch: [4]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 0.8841 (0.9886)  loss_classifier: 0.3534 (0.4010)  loss_box_reg: 0.4051 (0.4345)  loss_objectness: 0.0491 (0.0625)  loss_rpn_box_reg: 0.0523 (0.0907)  time: 0.4094  data: 0.0033  max mem: 3277\n",
            "Epoch: [4]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.9322 (0.9851)  loss_classifier: 0.3884 (0.3990)  loss_box_reg: 0.4200 (0.4338)  loss_objectness: 0.0509 (0.0621)  loss_rpn_box_reg: 0.0843 (0.0902)  time: 0.4096  data: 0.0033  max mem: 3277\n",
            "Epoch: [4]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.9055 (0.9807)  loss_classifier: 0.3454 (0.3964)  loss_box_reg: 0.3999 (0.4327)  loss_objectness: 0.0513 (0.0617)  loss_rpn_box_reg: 0.0773 (0.0899)  time: 0.4046  data: 0.0033  max mem: 3277\n",
            "Epoch: [4] Total time: 0:08:10 (0.4216 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:45  model_time: 0.4840 (0.4840)  evaluator_time: 0.0070 (0.0070)  time: 10.3812  data: 9.8262  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1850 (0.1860)  evaluator_time: 0.0110 (0.0109)  time: 0.2067  data: 0.0030  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1840 (0.1850)  evaluator_time: 0.0090 (0.0114)  time: 0.2045  data: 0.0029  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2962 s / it)\n",
            "Epoch: [5]  [   0/1164]  eta: 3:40:49  lr: 0.001000  loss: 1.2012 (1.2012)  loss_classifier: 0.5229 (0.5229)  loss_box_reg: 0.4546 (0.4546)  loss_objectness: 0.0939 (0.0939)  loss_rpn_box_reg: 0.1299 (0.1299)  time: 11.3826  data: 10.6476  max mem: 3277\n",
            "Epoch: [5]  [ 100/1164]  eta: 0:09:10  lr: 0.001000  loss: 0.9133 (0.9415)  loss_classifier: 0.3710 (0.3799)  loss_box_reg: 0.4043 (0.4167)  loss_objectness: 0.0502 (0.0578)  loss_rpn_box_reg: 0.0578 (0.0871)  time: 0.4076  data: 0.0031  max mem: 3277\n",
            "Epoch: [5]  [ 200/1164]  eta: 0:07:26  lr: 0.001000  loss: 0.9274 (0.9440)  loss_classifier: 0.3452 (0.3740)  loss_box_reg: 0.4191 (0.4199)  loss_objectness: 0.0463 (0.0572)  loss_rpn_box_reg: 0.0698 (0.0930)  time: 0.4094  data: 0.0033  max mem: 3277\n",
            "Epoch: [5]  [ 300/1164]  eta: 0:06:24  lr: 0.001000  loss: 0.8577 (0.9374)  loss_classifier: 0.3558 (0.3710)  loss_box_reg: 0.4043 (0.4173)  loss_objectness: 0.0468 (0.0586)  loss_rpn_box_reg: 0.0616 (0.0905)  time: 0.4091  data: 0.0031  max mem: 3277\n",
            "Epoch: [5]  [ 400/1164]  eta: 0:05:33  lr: 0.001000  loss: 0.8341 (0.9265)  loss_classifier: 0.3206 (0.3641)  loss_box_reg: 0.4088 (0.4166)  loss_objectness: 0.0377 (0.0569)  loss_rpn_box_reg: 0.0616 (0.0889)  time: 0.4094  data: 0.0032  max mem: 3277\n",
            "Epoch: [5]  [ 500/1164]  eta: 0:04:46  lr: 0.001000  loss: 0.8808 (0.9227)  loss_classifier: 0.3445 (0.3626)  loss_box_reg: 0.3857 (0.4147)  loss_objectness: 0.0429 (0.0568)  loss_rpn_box_reg: 0.0657 (0.0886)  time: 0.4083  data: 0.0031  max mem: 3277\n",
            "Epoch: [5]  [ 600/1164]  eta: 0:04:01  lr: 0.001000  loss: 0.8597 (0.9145)  loss_classifier: 0.3348 (0.3593)  loss_box_reg: 0.4206 (0.4131)  loss_objectness: 0.0398 (0.0554)  loss_rpn_box_reg: 0.0512 (0.0867)  time: 0.4084  data: 0.0031  max mem: 3277\n",
            "Epoch: [5]  [ 700/1164]  eta: 0:03:17  lr: 0.001000  loss: 0.8482 (0.9148)  loss_classifier: 0.3084 (0.3581)  loss_box_reg: 0.4000 (0.4116)  loss_objectness: 0.0461 (0.0560)  loss_rpn_box_reg: 0.0690 (0.0892)  time: 0.4080  data: 0.0031  max mem: 3277\n",
            "Epoch: [5]  [ 800/1164]  eta: 0:02:33  lr: 0.001000  loss: 0.8913 (0.9106)  loss_classifier: 0.3348 (0.3576)  loss_box_reg: 0.4053 (0.4098)  loss_objectness: 0.0454 (0.0554)  loss_rpn_box_reg: 0.0625 (0.0878)  time: 0.4086  data: 0.0031  max mem: 3277\n",
            "Epoch: [5]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 0.8476 (0.9039)  loss_classifier: 0.3487 (0.3549)  loss_box_reg: 0.3901 (0.4074)  loss_objectness: 0.0560 (0.0546)  loss_rpn_box_reg: 0.0726 (0.0870)  time: 0.4087  data: 0.0032  max mem: 3277\n",
            "Epoch: [5]  [1000/1164]  eta: 0:01:08  lr: 0.001000  loss: 0.9044 (0.9014)  loss_classifier: 0.3420 (0.3530)  loss_box_reg: 0.4008 (0.4062)  loss_objectness: 0.0502 (0.0543)  loss_rpn_box_reg: 0.0768 (0.0878)  time: 0.4102  data: 0.0032  max mem: 3277\n",
            "Epoch: [5]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.7613 (0.8977)  loss_classifier: 0.3013 (0.3509)  loss_box_reg: 0.3946 (0.4052)  loss_objectness: 0.0458 (0.0539)  loss_rpn_box_reg: 0.0563 (0.0878)  time: 0.4068  data: 0.0032  max mem: 3277\n",
            "Epoch: [5]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.8722 (0.8969)  loss_classifier: 0.3237 (0.3496)  loss_box_reg: 0.4001 (0.4049)  loss_objectness: 0.0522 (0.0542)  loss_rpn_box_reg: 0.0571 (0.0882)  time: 0.4053  data: 0.0031  max mem: 3277\n",
            "Epoch: [5] Total time: 0:08:07 (0.4190 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:42  model_time: 0.3909 (0.3909)  evaluator_time: 0.0070 (0.0070)  time: 10.3583  data: 9.8994  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1840 (0.1852)  evaluator_time: 0.0100 (0.0114)  time: 0.2069  data: 0.0029  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1850 (0.1843)  evaluator_time: 0.0100 (0.0112)  time: 0.2012  data: 0.0029  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2959 s / it)\n",
            "Epoch: [6]  [   0/1164]  eta: 3:29:57  lr: 0.001000  loss: 0.5138 (0.5138)  loss_classifier: 0.2084 (0.2084)  loss_box_reg: 0.2681 (0.2681)  loss_objectness: 0.0248 (0.0248)  loss_rpn_box_reg: 0.0126 (0.0126)  time: 10.8230  data: 10.1660  max mem: 3277\n",
            "Epoch: [6]  [ 100/1164]  eta: 0:09:02  lr: 0.001000  loss: 0.8633 (0.8538)  loss_classifier: 0.3419 (0.3279)  loss_box_reg: 0.3985 (0.3883)  loss_objectness: 0.0529 (0.0489)  loss_rpn_box_reg: 0.0804 (0.0887)  time: 0.4072  data: 0.0032  max mem: 3277\n",
            "Epoch: [6]  [ 200/1164]  eta: 0:07:23  lr: 0.001000  loss: 0.7591 (0.8407)  loss_classifier: 0.2915 (0.3232)  loss_box_reg: 0.3674 (0.3840)  loss_objectness: 0.0408 (0.0502)  loss_rpn_box_reg: 0.0550 (0.0832)  time: 0.4106  data: 0.0041  max mem: 3277\n",
            "Epoch: [6]  [ 300/1164]  eta: 0:06:23  lr: 0.001000  loss: 0.8292 (0.8488)  loss_classifier: 0.3046 (0.3267)  loss_box_reg: 0.3840 (0.3889)  loss_objectness: 0.0473 (0.0500)  loss_rpn_box_reg: 0.0624 (0.0832)  time: 0.4102  data: 0.0042  max mem: 3277\n",
            "Epoch: [6]  [ 400/1164]  eta: 0:05:33  lr: 0.001000  loss: 0.8502 (0.8493)  loss_classifier: 0.3213 (0.3260)  loss_box_reg: 0.3831 (0.3879)  loss_objectness: 0.0373 (0.0502)  loss_rpn_box_reg: 0.0664 (0.0851)  time: 0.4132  data: 0.0045  max mem: 3277\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "│    │    └─Linear (fc6)                                [4000, 1024]              12,846,080\n",
            "│    │    └─Linear (fc7)                                [4000, 1024]              1,049,600\n",
            "│    └─FastRCNNPredictor (box_predictor)                [4000, 31]                --\n",
            "│    │    └─Linear (cls_score)                          [4000, 31]                31,775\n",
            "│    │    └─Linear (bbox_pred)                          [4000, 124]               127,100\n",
            "=========================================================================================================\n",
            "Total params: 41,447,786\n",
            "Trainable params: 41,225,386\n",
            "Non-trainable params: 222,400\n",
            "Total mult-adds (G): 536.47\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.66\n",
            "Forward/backward pass size (MB): 5939.38\n",
            "Params size (MB): 165.79\n",
            "Estimated Total Size (MB): 6124.83\n",
            "=========================================================================================================\n",
            "41,447,786 total parameters.\n",
            "41,225,386 training parameters.\n",
            "creating index...\n",
            "index created!\n",
            "Averaged stats: model_time: 0.1873 (0.1906)  evaluator_time: 0.0140 (0.0154)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.57s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.068\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.032\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.030\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.097\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.176\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "\n",
            "BEST VALIDATION mAP: 0.024543885784906603\n",
            "\n",
            "SAVING BEST MODEL FOR EPOCH: 1\n",
            "\n",
            "creating index...\n",
            "index created!\n",
            "Averaged stats: model_time: 0.1860 (0.1877)  evaluator_time: 0.0130 (0.0144)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.54s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.073\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.055\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.085\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.079\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.111\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.255\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.326\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "\n",
            "BEST VALIDATION mAP: 0.0733009185827835\n",
            "\n",
            "SAVING BEST MODEL FOR EPOCH: 2\n",
            "\n",
            "creating index...\n",
            "index created!\n",
            "Averaged stats: model_time: 0.1870 (0.1874)  evaluator_time: 0.0120 (0.0143)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.52s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.085\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.176\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.077\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.093\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.094\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.332\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.349\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.353\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "SAVING PLOTS COMPLETE...\n",
            "\n",
            "BEST VALIDATION mAP: 0.08530267574292444\n",
            "\n",
            "SAVING BEST MODEL FOR EPOCH: 3\n",
            "\n",
            "creating index...\n",
            "index created!\n",
            "Averaged stats: model_time: 0.1860 (0.1853)  evaluator_time: 0.0110 (0.0123)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.44s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.097\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.077\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.098\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.362\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: [6]  [ 500/1164]  eta: 0:04:46  lr: 0.001000  loss: 0.7941 (0.8470)  loss_classifier: 0.2983 (0.3244)  loss_box_reg: 0.3806 (0.3881)  loss_objectness: 0.0525 (0.0507)  loss_rpn_box_reg: 0.0545 (0.0838)  time: 0.4115  data: 0.0043  max mem: 3277\n",
            "Epoch: [6]  [ 600/1164]  eta: 0:04:01  lr: 0.001000  loss: 0.8809 (0.8504)  loss_classifier: 0.3179 (0.3242)  loss_box_reg: 0.3961 (0.3890)  loss_objectness: 0.0461 (0.0515)  loss_rpn_box_reg: 0.0705 (0.0857)  time: 0.4115  data: 0.0043  max mem: 3277\n",
            "Epoch: [6]  [ 700/1164]  eta: 0:03:17  lr: 0.001000  loss: 0.8551 (0.8487)  loss_classifier: 0.3019 (0.3215)  loss_box_reg: 0.3642 (0.3876)  loss_objectness: 0.0476 (0.0517)  loss_rpn_box_reg: 0.0855 (0.0878)  time: 0.4115  data: 0.0044  max mem: 3277\n",
            "Epoch: [6]  [ 800/1164]  eta: 0:02:34  lr: 0.001000  loss: 0.7854 (0.8435)  loss_classifier: 0.2748 (0.3180)  loss_box_reg: 0.3802 (0.3865)  loss_objectness: 0.0396 (0.0515)  loss_rpn_box_reg: 0.0531 (0.0875)  time: 0.4122  data: 0.0038  max mem: 3277\n",
            "Epoch: [6]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 0.7834 (0.8402)  loss_classifier: 0.2904 (0.3166)  loss_box_reg: 0.3717 (0.3859)  loss_objectness: 0.0459 (0.0509)  loss_rpn_box_reg: 0.0544 (0.0868)  time: 0.4122  data: 0.0044  max mem: 3277\n",
            "Epoch: [6]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 0.8061 (0.8381)  loss_classifier: 0.2887 (0.3156)  loss_box_reg: 0.3806 (0.3856)  loss_objectness: 0.0442 (0.0506)  loss_rpn_box_reg: 0.0618 (0.0863)  time: 0.4105  data: 0.0041  max mem: 3277\n",
            "Epoch: [6]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.6814 (0.8345)  loss_classifier: 0.2562 (0.3144)  loss_box_reg: 0.3171 (0.3843)  loss_objectness: 0.0332 (0.0500)  loss_rpn_box_reg: 0.0467 (0.0857)  time: 0.4108  data: 0.0040  max mem: 3277\n",
            "Epoch: [6]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.7742 (0.8326)  loss_classifier: 0.2919 (0.3142)  loss_box_reg: 0.3723 (0.3840)  loss_objectness: 0.0414 (0.0498)  loss_rpn_box_reg: 0.0465 (0.0846)  time: 0.4061  data: 0.0038  max mem: 3277\n",
            "Epoch: [6] Total time: 0:08:09 (0.4206 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:57  model_time: 0.5240 (0.5240)  evaluator_time: 0.0060 (0.0060)  time: 10.4812  data: 9.8881  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1838 (0.1849)  evaluator_time: 0.0090 (0.0107)  time: 0.2055  data: 0.0030  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1850 (0.1841)  evaluator_time: 0.0090 (0.0105)  time: 0.2004  data: 0.0029  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2951 s / it)\n",
            "Epoch: [7]  [   0/1164]  eta: 3:38:06  lr: 0.001000  loss: 0.8767 (0.8767)  loss_classifier: 0.2788 (0.2788)  loss_box_reg: 0.3806 (0.3806)  loss_objectness: 0.0757 (0.0757)  loss_rpn_box_reg: 0.1416 (0.1416)  time: 11.2426  data: 10.4906  max mem: 3277\n",
            "Epoch: [7]  [ 100/1164]  eta: 0:09:05  lr: 0.001000  loss: 0.8090 (0.8013)  loss_classifier: 0.2862 (0.2914)  loss_box_reg: 0.3741 (0.3744)  loss_objectness: 0.0441 (0.0488)  loss_rpn_box_reg: 0.0489 (0.0866)  time: 0.4058  data: 0.0032  max mem: 3277\n",
            "Epoch: [7]  [ 200/1164]  eta: 0:07:23  lr: 0.001000  loss: 0.7864 (0.7847)  loss_classifier: 0.2919 (0.2863)  loss_box_reg: 0.3938 (0.3739)  loss_objectness: 0.0443 (0.0473)  loss_rpn_box_reg: 0.0384 (0.0771)  time: 0.4081  data: 0.0033  max mem: 3277\n",
            "Epoch: [7]  [ 300/1164]  eta: 0:06:22  lr: 0.001000  loss: 0.7430 (0.7869)  loss_classifier: 0.2662 (0.2878)  loss_box_reg: 0.3622 (0.3722)  loss_objectness: 0.0366 (0.0465)  loss_rpn_box_reg: 0.0479 (0.0805)  time: 0.4073  data: 0.0031  max mem: 3277\n",
            "Epoch: [7]  [ 400/1164]  eta: 0:05:31  lr: 0.001000  loss: 0.7270 (0.7880)  loss_classifier: 0.2944 (0.2884)  loss_box_reg: 0.3664 (0.3712)  loss_objectness: 0.0353 (0.0464)  loss_rpn_box_reg: 0.0400 (0.0820)  time: 0.4078  data: 0.0032  max mem: 3277\n",
            "Epoch: [7]  [ 500/1164]  eta: 0:04:44  lr: 0.001000  loss: 0.7917 (0.7841)  loss_classifier: 0.2931 (0.2885)  loss_box_reg: 0.3767 (0.3695)  loss_objectness: 0.0382 (0.0458)  loss_rpn_box_reg: 0.0490 (0.0803)  time: 0.4092  data: 0.0032  max mem: 3277\n",
            "Epoch: [7]  [ 600/1164]  eta: 0:03:59  lr: 0.001000  loss: 0.8219 (0.7837)  loss_classifier: 0.3086 (0.2884)  loss_box_reg: 0.3794 (0.3688)  loss_objectness: 0.0385 (0.0455)  loss_rpn_box_reg: 0.0952 (0.0811)  time: 0.4089  data: 0.0033  max mem: 3277\n",
            "Epoch: [7]  [ 700/1164]  eta: 0:03:16  lr: 0.001000  loss: 0.7116 (0.7824)  loss_classifier: 0.2726 (0.2882)  loss_box_reg: 0.3444 (0.3677)  loss_objectness: 0.0387 (0.0454)  loss_rpn_box_reg: 0.0684 (0.0810)  time: 0.4068  data: 0.0031  max mem: 3277\n",
            "Epoch: [7]  [ 800/1164]  eta: 0:02:33  lr: 0.001000  loss: 0.7278 (0.7841)  loss_classifier: 0.2895 (0.2891)  loss_box_reg: 0.3566 (0.3682)  loss_objectness: 0.0338 (0.0455)  loss_rpn_box_reg: 0.0436 (0.0813)  time: 0.4080  data: 0.0031  max mem: 3277\n",
            "Epoch: [7]  [ 900/1164]  eta: 0:01:50  lr: 0.001000  loss: 0.7317 (0.7826)  loss_classifier: 0.2720 (0.2891)  loss_box_reg: 0.3471 (0.3674)  loss_objectness: 0.0465 (0.0461)  loss_rpn_box_reg: 0.0493 (0.0799)  time: 0.4074  data: 0.0031  max mem: 3277\n",
            "Epoch: [7]  [1000/1164]  eta: 0:01:08  lr: 0.001000  loss: 0.7255 (0.7800)  loss_classifier: 0.2533 (0.2879)  loss_box_reg: 0.3542 (0.3659)  loss_objectness: 0.0361 (0.0458)  loss_rpn_box_reg: 0.0493 (0.0804)  time: 0.4098  data: 0.0032  max mem: 3277\n",
            "Epoch: [7]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.7116 (0.7780)  loss_classifier: 0.2519 (0.2866)  loss_box_reg: 0.3443 (0.3654)  loss_objectness: 0.0333 (0.0457)  loss_rpn_box_reg: 0.0507 (0.0803)  time: 0.4075  data: 0.0031  max mem: 3277\n",
            "Epoch: [7]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.7396 (0.7770)  loss_classifier: 0.2510 (0.2859)  loss_box_reg: 0.3411 (0.3648)  loss_objectness: 0.0376 (0.0457)  loss_rpn_box_reg: 0.0530 (0.0807)  time: 0.4041  data: 0.0031  max mem: 3277\n",
            "Epoch: [7] Total time: 0:08:06 (0.4176 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:26  model_time: 0.4270 (0.4270)  evaluator_time: 0.0050 (0.0050)  time: 10.2218  data: 9.7238  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1820 (0.1832)  evaluator_time: 0.0090 (0.0103)  time: 0.2039  data: 0.0030  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1830 (0.1824)  evaluator_time: 0.0090 (0.0108)  time: 0.2026  data: 0.0029  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2921 s / it)\n",
            "Epoch: [8]  [   0/1164]  eta: 3:37:09  lr: 0.001000  loss: 1.0487 (1.0487)  loss_classifier: 0.4015 (0.4015)  loss_box_reg: 0.4861 (0.4861)  loss_objectness: 0.0628 (0.0628)  loss_rpn_box_reg: 0.0984 (0.0984)  time: 11.1936  data: 10.5646  max mem: 3277\n",
            "Epoch: [8]  [ 100/1164]  eta: 0:09:06  lr: 0.001000  loss: 0.6757 (0.7578)  loss_classifier: 0.2427 (0.2754)  loss_box_reg: 0.3456 (0.3579)  loss_objectness: 0.0327 (0.0471)  loss_rpn_box_reg: 0.0373 (0.0775)  time: 0.4075  data: 0.0032  max mem: 3277\n",
            "Epoch: [8]  [ 200/1164]  eta: 0:07:24  lr: 0.001000  loss: 0.7602 (0.7473)  loss_classifier: 0.2578 (0.2714)  loss_box_reg: 0.3697 (0.3551)  loss_objectness: 0.0428 (0.0455)  loss_rpn_box_reg: 0.0682 (0.0753)  time: 0.4103  data: 0.0044  max mem: 3277\n",
            "Epoch: [8]  [ 300/1164]  eta: 0:06:24  lr: 0.001000  loss: 0.7131 (0.7441)  loss_classifier: 0.2413 (0.2699)  loss_box_reg: 0.3291 (0.3520)  loss_objectness: 0.0317 (0.0451)  loss_rpn_box_reg: 0.0626 (0.0771)  time: 0.4110  data: 0.0044  max mem: 3277\n",
            "Epoch: [8]  [ 400/1164]  eta: 0:05:33  lr: 0.001000  loss: 0.6851 (0.7403)  loss_classifier: 0.2609 (0.2659)  loss_box_reg: 0.3494 (0.3520)  loss_objectness: 0.0353 (0.0441)  loss_rpn_box_reg: 0.0551 (0.0782)  time: 0.4119  data: 0.0041  max mem: 3277\n",
            "Epoch: [8]  [ 500/1164]  eta: 0:04:46  lr: 0.001000  loss: 0.7581 (0.7356)  loss_classifier: 0.2724 (0.2641)  loss_box_reg: 0.3852 (0.3515)  loss_objectness: 0.0403 (0.0434)  loss_rpn_box_reg: 0.0440 (0.0766)  time: 0.4123  data: 0.0044  max mem: 3277\n",
            "Epoch: [8]  [ 600/1164]  eta: 0:04:01  lr: 0.001000  loss: 0.6659 (0.7337)  loss_classifier: 0.2354 (0.2638)  loss_box_reg: 0.3435 (0.3490)  loss_objectness: 0.0342 (0.0431)  loss_rpn_box_reg: 0.0375 (0.0777)  time: 0.4122  data: 0.0042  max mem: 3277\n",
            "Epoch: [8]  [ 700/1164]  eta: 0:03:17  lr: 0.001000  loss: 0.6963 (0.7307)  loss_classifier: 0.2527 (0.2620)  loss_box_reg: 0.3327 (0.3478)  loss_objectness: 0.0416 (0.0426)  loss_rpn_box_reg: 0.0406 (0.0783)  time: 0.4097  data: 0.0042  max mem: 3277\n",
            "Epoch: [8]  [ 800/1164]  eta: 0:02:34  lr: 0.001000  loss: 0.6815 (0.7304)  loss_classifier: 0.2446 (0.2613)  loss_box_reg: 0.3160 (0.3483)  loss_objectness: 0.0355 (0.0423)  loss_rpn_box_reg: 0.0451 (0.0785)  time: 0.4123  data: 0.0042  max mem: 3277\n",
            "Epoch: [8]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 0.7568 (0.7293)  loss_classifier: 0.2469 (0.2607)  loss_box_reg: 0.3458 (0.3477)  loss_objectness: 0.0386 (0.0422)  loss_rpn_box_reg: 0.0710 (0.0788)  time: 0.4127  data: 0.0044  max mem: 3277\n",
            "Epoch: [8]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 0.6714 (0.7270)  loss_classifier: 0.2386 (0.2594)  loss_box_reg: 0.3200 (0.3469)  loss_objectness: 0.0426 (0.0423)  loss_rpn_box_reg: 0.0679 (0.0784)  time: 0.4145  data: 0.0046  max mem: 3277\n",
            "Epoch: [8]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.6386 (0.7260)  loss_classifier: 0.2393 (0.2590)  loss_box_reg: 0.3171 (0.3468)  loss_objectness: 0.0348 (0.0420)  loss_rpn_box_reg: 0.0472 (0.0781)  time: 0.4117  data: 0.0042  max mem: 3277\n",
            "Epoch: [8]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.6955 (0.7242)  loss_classifier: 0.2308 (0.2577)  loss_box_reg: 0.3432 (0.3464)  loss_objectness: 0.0306 (0.0419)  loss_rpn_box_reg: 0.0661 (0.0782)  time: 0.4076  data: 0.0040  max mem: 3277\n",
            "Epoch: [8] Total time: 0:08:10 (0.4213 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:48  model_time: 0.3640 (0.3640)  evaluator_time: 0.0050 (0.0050)  time: 10.4016  data: 9.9676  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1840 (0.1837)  evaluator_time: 0.0090 (0.0104)  time: 0.2060  data: 0.0030  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1840 (0.1830)  evaluator_time: 0.0090 (0.0102)  time: 0.2000  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2952 s / it)\n",
            "Epoch: [9]  [   0/1164]  eta: 3:32:26  lr: 0.001000  loss: 0.6953 (0.6953)  loss_classifier: 0.2327 (0.2327)  loss_box_reg: 0.3415 (0.3415)  loss_objectness: 0.0408 (0.0408)  loss_rpn_box_reg: 0.0802 (0.0802)  time: 10.9506  data: 10.3456  max mem: 3277\n",
            "Epoch: [9]  [ 100/1164]  eta: 0:09:04  lr: 0.001000  loss: 0.6837 (0.7135)  loss_classifier: 0.2336 (0.2471)  loss_box_reg: 0.3573 (0.3435)  loss_objectness: 0.0307 (0.0413)  loss_rpn_box_reg: 0.0613 (0.0816)  time: 0.4076  data: 0.0033  max mem: 3277\n",
            "Epoch: [9]  [ 200/1164]  eta: 0:07:24  lr: 0.001000  loss: 0.7728 (0.7156)  loss_classifier: 0.2624 (0.2486)  loss_box_reg: 0.3588 (0.3453)  loss_objectness: 0.0477 (0.0418)  loss_rpn_box_reg: 0.0770 (0.0798)  time: 0.4129  data: 0.0041  max mem: 3277\n",
            "Epoch: [9]  [ 300/1164]  eta: 0:06:24  lr: 0.001000  loss: 0.7925 (0.7149)  loss_classifier: 0.2720 (0.2495)  loss_box_reg: 0.3515 (0.3440)  loss_objectness: 0.0396 (0.0421)  loss_rpn_box_reg: 0.0604 (0.0793)  time: 0.4117  data: 0.0041  max mem: 3277\n",
            "Epoch: [9]  [ 400/1164]  eta: 0:05:33  lr: 0.001000  loss: 0.7073 (0.7141)  loss_classifier: 0.2549 (0.2479)  loss_box_reg: 0.3479 (0.3420)  loss_objectness: 0.0391 (0.0417)  loss_rpn_box_reg: 0.0683 (0.0825)  time: 0.4115  data: 0.0039  max mem: 3277\n",
            "Epoch: [9]  [ 500/1164]  eta: 0:04:46  lr: 0.001000  loss: 0.7149 (0.7135)  loss_classifier: 0.2418 (0.2466)  loss_box_reg: 0.3339 (0.3414)  loss_objectness: 0.0366 (0.0423)  loss_rpn_box_reg: 0.0995 (0.0832)  time: 0.4116  data: 0.0042  max mem: 3277\n",
            "Epoch: [9]  [ 600/1164]  eta: 0:04:01  lr: 0.001000  loss: 0.6849 (0.7067)  loss_classifier: 0.2386 (0.2451)  loss_box_reg: 0.3253 (0.3404)  loss_objectness: 0.0316 (0.0413)  loss_rpn_box_reg: 0.0590 (0.0799)  time: 0.4122  data: 0.0044  max mem: 3277\n",
            "Epoch: [9]  [ 700/1164]  eta: 0:03:17  lr: 0.001000  loss: 0.7008 (0.7039)  loss_classifier: 0.2342 (0.2451)  loss_box_reg: 0.3263 (0.3387)  loss_objectness: 0.0359 (0.0408)  loss_rpn_box_reg: 0.0823 (0.0793)  time: 0.4109  data: 0.0041  max mem: 3277\n",
            "Epoch: [9]  [ 800/1164]  eta: 0:02:34  lr: 0.001000  loss: 0.6089 (0.7015)  loss_classifier: 0.2108 (0.2441)  loss_box_reg: 0.3039 (0.3373)  loss_objectness: 0.0337 (0.0408)  loss_rpn_box_reg: 0.0533 (0.0793)  time: 0.4106  data: 0.0040  max mem: 3277\n",
            "Epoch: [9]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 0.6727 (0.7010)  loss_classifier: 0.2425 (0.2437)  loss_box_reg: 0.3120 (0.3373)  loss_objectness: 0.0341 (0.0410)  loss_rpn_box_reg: 0.0824 (0.0791)  time: 0.4125  data: 0.0044  max mem: 3277\n",
            "Epoch: [9]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 0.6616 (0.7003)  loss_classifier: 0.1993 (0.2426)  loss_box_reg: 0.3239 (0.3364)  loss_objectness: 0.0490 (0.0413)  loss_rpn_box_reg: 0.0427 (0.0799)  time: 0.4211  data: 0.0045  max mem: 3277\n",
            "Epoch: [9]  [1100/1164]  eta: 0:00:27  lr: 0.001000  loss: 0.6311 (0.6983)  loss_classifier: 0.2053 (0.2422)  loss_box_reg: 0.3153 (0.3360)  loss_objectness: 0.0376 (0.0413)  loss_rpn_box_reg: 0.0399 (0.0789)  time: 0.4248  data: 0.0050  max mem: 3277\n",
            "Epoch: [9]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.5957 (0.6968)  loss_classifier: 0.2136 (0.2421)  loss_box_reg: 0.3260 (0.3352)  loss_objectness: 0.0303 (0.0410)  loss_rpn_box_reg: 0.0376 (0.0784)  time: 0.4106  data: 0.0040  max mem: 3277\n",
            "Epoch: [9] Total time: 0:08:13 (0.4236 s / it)\n",
            "Test:  [  0/120]  eta: 0:26:32  model_time: 0.5542 (0.5542)  evaluator_time: 0.0030 (0.0030)  time: 13.2693  data: 12.6461  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1830 (0.1883)  evaluator_time: 0.0090 (0.0099)  time: 0.2034  data: 0.0030  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1830 (0.1868)  evaluator_time: 0.0080 (0.0097)  time: 0.1982  data: 0.0028  max mem: 3277\n",
            "Test: Total time: 0:00:38 (0.3209 s / it)\n",
            "Epoch: [10]  [   0/1164]  eta: 4:18:29  lr: 0.001000  loss: 0.5179 (0.5179)  loss_classifier: 0.1715 (0.1715)  loss_box_reg: 0.2965 (0.2965)  loss_objectness: 0.0141 (0.0141)  loss_rpn_box_reg: 0.0359 (0.0359)  time: 13.3246  data: 12.5475  max mem: 3277\n",
            "Epoch: [10]  [ 100/1164]  eta: 0:09:38  lr: 0.001000  loss: 0.6297 (0.6549)  loss_classifier: 0.2295 (0.2291)  loss_box_reg: 0.3121 (0.3203)  loss_objectness: 0.0270 (0.0337)  loss_rpn_box_reg: 0.0322 (0.0719)  time: 0.4059  data: 0.0030  max mem: 3277\n",
            "Epoch: [10]  [ 200/1164]  eta: 0:07:38  lr: 0.001000  loss: 0.7339 (0.6774)  loss_classifier: 0.2436 (0.2354)  loss_box_reg: 0.3558 (0.3252)  loss_objectness: 0.0405 (0.0362)  loss_rpn_box_reg: 0.0686 (0.0805)  time: 0.4077  data: 0.0031  max mem: 3277\n",
            "Epoch: [10]  [ 300/1164]  eta: 0:06:31  lr: 0.001000  loss: 0.6020 (0.6668)  loss_classifier: 0.2009 (0.2327)  loss_box_reg: 0.3042 (0.3233)  loss_objectness: 0.0253 (0.0358)  loss_rpn_box_reg: 0.0351 (0.0749)  time: 0.4085  data: 0.0035  max mem: 3277\n",
            "Epoch: [10]  [ 400/1164]  eta: 0:05:37  lr: 0.001000  loss: 0.5965 (0.6647)  loss_classifier: 0.2228 (0.2302)  loss_box_reg: 0.3071 (0.3213)  loss_objectness: 0.0384 (0.0371)  loss_rpn_box_reg: 0.0496 (0.0761)  time: 0.4080  data: 0.0032  max mem: 3277\n",
            "Epoch: [10]  [ 500/1164]  eta: 0:04:49  lr: 0.001000  loss: 0.6093 (0.6591)  loss_classifier: 0.2176 (0.2280)  loss_box_reg: 0.3155 (0.3200)  loss_objectness: 0.0335 (0.0366)  loss_rpn_box_reg: 0.0313 (0.0745)  time: 0.4078  data: 0.0032  max mem: 3277\n",
            "Epoch: [10]  [ 600/1164]  eta: 0:04:02  lr: 0.001000  loss: 0.6471 (0.6574)  loss_classifier: 0.2329 (0.2278)  loss_box_reg: 0.2958 (0.3188)  loss_objectness: 0.0273 (0.0361)  loss_rpn_box_reg: 0.0498 (0.0747)  time: 0.4074  data: 0.0033  max mem: 3277\n",
            "Epoch: [10]  [ 700/1164]  eta: 0:03:18  lr: 0.001000  loss: 0.6766 (0.6572)  loss_classifier: 0.2437 (0.2276)  loss_box_reg: 0.3275 (0.3193)  loss_objectness: 0.0282 (0.0360)  loss_rpn_box_reg: 0.0451 (0.0743)  time: 0.4092  data: 0.0032  max mem: 3277\n",
            "Epoch: [10]  [ 800/1164]  eta: 0:02:34  lr: 0.001000  loss: 0.5518 (0.6545)  loss_classifier: 0.2002 (0.2267)  loss_box_reg: 0.3035 (0.3186)  loss_objectness: 0.0296 (0.0357)  loss_rpn_box_reg: 0.0380 (0.0735)  time: 0.4081  data: 0.0031  max mem: 3277\n",
            "Epoch: [10]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 0.6706 (0.6548)  loss_classifier: 0.2183 (0.2260)  loss_box_reg: 0.3345 (0.3184)  loss_objectness: 0.0392 (0.0362)  loss_rpn_box_reg: 0.0492 (0.0743)  time: 0.4077  data: 0.0031  max mem: 3277\n",
            "Epoch: [10]  [1000/1164]  eta: 0:01:09  lr: 0.001000  loss: 0.6219 (0.6522)  loss_classifier: 0.2301 (0.2248)  loss_box_reg: 0.3144 (0.3177)  loss_objectness: 0.0302 (0.0358)  loss_rpn_box_reg: 0.0433 (0.0738)  time: 0.4091  data: 0.0032  max mem: 3277\n",
            "Epoch: [10]  [1100/1164]  eta: 0:00:27  lr: 0.001000  loss: 0.6233 (0.6507)  loss_classifier: 0.1913 (0.2241)  loss_box_reg: 0.3064 (0.3170)  loss_objectness: 0.0258 (0.0357)  loss_rpn_box_reg: 0.0517 (0.0738)  time: 0.4213  data: 0.0041  max mem: 3277\n",
            "Epoch: [10]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.6124 (0.6492)  loss_classifier: 0.2130 (0.2236)  loss_box_reg: 0.3179 (0.3163)  loss_objectness: 0.0299 (0.0356)  loss_rpn_box_reg: 0.0466 (0.0737)  time: 0.4212  data: 0.0041  max mem: 3277\n",
            "Epoch: [10] Total time: 0:08:12 (0.4227 s / it)\n",
            "Test:  [  0/120]  eta: 0:24:28  model_time: 0.4600 (0.4600)  evaluator_time: 0.0040 (0.0040)  time: 12.2389  data: 11.7119  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1830 (0.1841)  evaluator_time: 0.0080 (0.0097)  time: 0.2032  data: 0.0028  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1840 (0.1832)  evaluator_time: 0.0080 (0.0096)  time: 0.1986  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:37 (0.3084 s / it)\n",
            "Epoch: [11]  [   0/1164]  eta: 3:37:19  lr: 0.001000  loss: 0.6315 (0.6315)  loss_classifier: 0.1995 (0.1995)  loss_box_reg: 0.3408 (0.3408)  loss_objectness: 0.0261 (0.0261)  loss_rpn_box_reg: 0.0650 (0.0650)  time: 11.2024  data: 10.5027  max mem: 3277\n",
            "Epoch: [11]  [ 100/1164]  eta: 0:09:06  lr: 0.001000  loss: 0.6605 (0.6829)  loss_classifier: 0.2090 (0.2343)  loss_box_reg: 0.3099 (0.3244)  loss_objectness: 0.0360 (0.0394)  loss_rpn_box_reg: 0.0824 (0.0847)  time: 0.4065  data: 0.0031  max mem: 3277\n",
            "Epoch: [11]  [ 200/1164]  eta: 0:07:24  lr: 0.001000  loss: 0.6671 (0.6589)  loss_classifier: 0.2263 (0.2248)  loss_box_reg: 0.3191 (0.3193)  loss_objectness: 0.0337 (0.0380)  loss_rpn_box_reg: 0.0502 (0.0769)  time: 0.4084  data: 0.0032  max mem: 3277\n",
            "Epoch: [11]  [ 300/1164]  eta: 0:06:22  lr: 0.001000  loss: 0.5940 (0.6469)  loss_classifier: 0.2113 (0.2202)  loss_box_reg: 0.3113 (0.3135)  loss_objectness: 0.0356 (0.0366)  loss_rpn_box_reg: 0.0342 (0.0766)  time: 0.4087  data: 0.0032  max mem: 3277\n",
            "Epoch: [11]  [ 400/1164]  eta: 0:05:31  lr: 0.001000  loss: 0.7294 (0.6415)  loss_classifier: 0.2446 (0.2181)  loss_box_reg: 0.3447 (0.3119)  loss_objectness: 0.0293 (0.0352)  loss_rpn_box_reg: 0.0603 (0.0761)  time: 0.4085  data: 0.0033  max mem: 3277\n",
            "Epoch: [11]  [ 500/1164]  eta: 0:04:44  lr: 0.001000  loss: 0.5897 (0.6401)  loss_classifier: 0.1997 (0.2174)  loss_box_reg: 0.2834 (0.3099)  loss_objectness: 0.0265 (0.0351)  loss_rpn_box_reg: 0.0618 (0.0778)  time: 0.4092  data: 0.0032  max mem: 3277\n",
            "Epoch: [11]  [ 600/1164]  eta: 0:03:59  lr: 0.001000  loss: 0.5817 (0.6386)  loss_classifier: 0.1978 (0.2163)  loss_box_reg: 0.3078 (0.3096)  loss_objectness: 0.0256 (0.0350)  loss_rpn_box_reg: 0.0411 (0.0777)  time: 0.4070  data: 0.0033  max mem: 3277\n",
            "Epoch: [11]  [ 700/1164]  eta: 0:03:16  lr: 0.001000  loss: 0.6308 (0.6354)  loss_classifier: 0.1980 (0.2151)  loss_box_reg: 0.3163 (0.3090)  loss_objectness: 0.0345 (0.0349)  loss_rpn_box_reg: 0.0575 (0.0764)  time: 0.4075  data: 0.0033  max mem: 3277\n",
            "Epoch: [11]  [ 800/1164]  eta: 0:02:33  lr: 0.001000  loss: 0.5687 (0.6331)  loss_classifier: 0.1951 (0.2144)  loss_box_reg: 0.2934 (0.3084)  loss_objectness: 0.0292 (0.0348)  loss_rpn_box_reg: 0.0519 (0.0754)  time: 0.4061  data: 0.0033  max mem: 3277\n",
            "Epoch: [11]  [ 900/1164]  eta: 0:01:50  lr: 0.001000  loss: 0.6386 (0.6342)  loss_classifier: 0.1968 (0.2148)  loss_box_reg: 0.3177 (0.3086)  loss_objectness: 0.0374 (0.0350)  loss_rpn_box_reg: 0.0624 (0.0758)  time: 0.4088  data: 0.0031  max mem: 3277\n",
            "Epoch: [11]  [1000/1164]  eta: 0:01:08  lr: 0.001000  loss: 0.5676 (0.6314)  loss_classifier: 0.2015 (0.2139)  loss_box_reg: 0.2951 (0.3085)  loss_objectness: 0.0245 (0.0347)  loss_rpn_box_reg: 0.0294 (0.0744)  time: 0.4092  data: 0.0033  max mem: 3277\n",
            "Epoch: [11]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.6412 (0.6281)  loss_classifier: 0.2098 (0.2124)  loss_box_reg: 0.3105 (0.3076)  loss_objectness: 0.0303 (0.0343)  loss_rpn_box_reg: 0.0709 (0.0737)  time: 0.4071  data: 0.0032  max mem: 3277\n",
            "Epoch: [11]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.5645 (0.6268)  loss_classifier: 0.1914 (0.2115)  loss_box_reg: 0.2889 (0.3070)  loss_objectness: 0.0338 (0.0343)  loss_rpn_box_reg: 0.0570 (0.0740)  time: 0.4039  data: 0.0033  max mem: 3277\n",
            "Epoch: [11] Total time: 0:08:05 (0.4173 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:39  model_time: 0.4410 (0.4410)  evaluator_time: 0.0040 (0.0040)  time: 10.3315  data: 9.8205  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1830 (0.1846)  evaluator_time: 0.0100 (0.0098)  time: 0.2039  data: 0.0029  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1840 (0.1838)  evaluator_time: 0.0090 (0.0098)  time: 0.2000  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2933 s / it)\n",
            "Epoch: [12]  [   0/1164]  eta: 3:28:53  lr: 0.001000  loss: 0.4104 (0.4104)  loss_classifier: 0.1325 (0.1325)  loss_box_reg: 0.2232 (0.2232)  loss_objectness: 0.0340 (0.0340)  loss_rpn_box_reg: 0.0206 (0.0206)  time: 10.7678  data: 10.1188  max mem: 3277\n",
            "Epoch: [12]  [ 100/1164]  eta: 0:09:01  lr: 0.001000  loss: 0.5025 (0.5712)  loss_classifier: 0.1690 (0.1951)  loss_box_reg: 0.2888 (0.2932)  loss_objectness: 0.0226 (0.0283)  loss_rpn_box_reg: 0.0272 (0.0546)  time: 0.4070  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [ 200/1164]  eta: 0:07:21  lr: 0.001000  loss: 0.6196 (0.5899)  loss_classifier: 0.2200 (0.2017)  loss_box_reg: 0.3197 (0.2975)  loss_objectness: 0.0263 (0.0295)  loss_rpn_box_reg: 0.0457 (0.0611)  time: 0.4088  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [ 300/1164]  eta: 0:06:21  lr: 0.001000  loss: 0.5869 (0.5975)  loss_classifier: 0.2229 (0.2035)  loss_box_reg: 0.2953 (0.2971)  loss_objectness: 0.0245 (0.0309)  loss_rpn_box_reg: 0.0540 (0.0660)  time: 0.4103  data: 0.0031  max mem: 3277\n",
            "Epoch: [12]  [ 400/1164]  eta: 0:05:31  lr: 0.001000  loss: 0.5696 (0.5934)  loss_classifier: 0.2226 (0.2013)  loss_box_reg: 0.3051 (0.2956)  loss_objectness: 0.0264 (0.0306)  loss_rpn_box_reg: 0.0486 (0.0659)  time: 0.4088  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [ 500/1164]  eta: 0:04:44  lr: 0.001000  loss: 0.6093 (0.5912)  loss_classifier: 0.1920 (0.2010)  loss_box_reg: 0.2804 (0.2950)  loss_objectness: 0.0331 (0.0301)  loss_rpn_box_reg: 0.0631 (0.0650)  time: 0.4086  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [ 600/1164]  eta: 0:03:59  lr: 0.001000  loss: 0.5839 (0.5908)  loss_classifier: 0.1823 (0.2002)  loss_box_reg: 0.3047 (0.2952)  loss_objectness: 0.0310 (0.0305)  loss_rpn_box_reg: 0.0464 (0.0648)  time: 0.4084  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [ 700/1164]  eta: 0:03:16  lr: 0.001000  loss: 0.5387 (0.5893)  loss_classifier: 0.1924 (0.1984)  loss_box_reg: 0.2750 (0.2940)  loss_objectness: 0.0249 (0.0308)  loss_rpn_box_reg: 0.0332 (0.0662)  time: 0.4091  data: 0.0031  max mem: 3277\n",
            "Epoch: [12]  [ 800/1164]  eta: 0:02:33  lr: 0.001000  loss: 0.6119 (0.5889)  loss_classifier: 0.1997 (0.1979)  loss_box_reg: 0.3090 (0.2941)  loss_objectness: 0.0302 (0.0309)  loss_rpn_box_reg: 0.0481 (0.0661)  time: 0.4091  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [ 900/1164]  eta: 0:01:50  lr: 0.001000  loss: 0.5770 (0.5898)  loss_classifier: 0.1899 (0.1974)  loss_box_reg: 0.2833 (0.2936)  loss_objectness: 0.0236 (0.0310)  loss_rpn_box_reg: 0.0729 (0.0679)  time: 0.4091  data: 0.0032  max mem: 3277\n",
            "Epoch: [12]  [1000/1164]  eta: 0:01:08  lr: 0.001000  loss: 0.5527 (0.5910)  loss_classifier: 0.2045 (0.1972)  loss_box_reg: 0.2915 (0.2935)  loss_objectness: 0.0259 (0.0321)  loss_rpn_box_reg: 0.0452 (0.0682)  time: 0.4090  data: 0.0031  max mem: 3277\n",
            "Epoch: [12]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.5865 (0.5907)  loss_classifier: 0.1878 (0.1967)  loss_box_reg: 0.2737 (0.2933)  loss_objectness: 0.0290 (0.0323)  loss_rpn_box_reg: 0.0636 (0.0685)  time: 0.4084  data: 0.0034  max mem: 3277\n",
            "Epoch: [12]  [1163/1164]  eta: 0:00:00  lr: 0.001000  loss: 0.5713 (0.5895)  loss_classifier: 0.1809 (0.1962)  loss_box_reg: 0.2775 (0.2928)  loss_objectness: 0.0285 (0.0322)  loss_rpn_box_reg: 0.0432 (0.0682)  time: 0.4054  data: 0.0032  max mem: 3277\n",
            "Epoch: [12] Total time: 0:08:06 (0.4180 s / it)\n",
            "Test:  [  0/120]  eta: 0:20:52  model_time: 0.5260 (0.5260)  evaluator_time: 0.0040 (0.0040)  time: 10.4385  data: 9.8465  max mem: 3277\n",
            "Test:  [100/120]  eta: 0:00:06  model_time: 0.1820 (0.1842)  evaluator_time: 0.0090 (0.0092)  time: 0.2034  data: 0.0029  max mem: 3277\n",
            "Test:  [119/120]  eta: 0:00:00  model_time: 0.1810 (0.1831)  evaluator_time: 0.0090 (0.0092)  time: 0.1979  data: 0.0030  max mem: 3277\n",
            "Test: Total time: 0:00:35 (0.2932 s / it)\n",
            "Epoch: [13]  [   0/1164]  eta: 3:34:00  lr: 0.001000  loss: 0.6986 (0.6986)  loss_classifier: 0.1981 (0.1981)  loss_box_reg: 0.2715 (0.2715)  loss_objectness: 0.0543 (0.0543)  loss_rpn_box_reg: 0.1747 (0.1747)  time: 11.0313  data: 10.3858  max mem: 3277\n",
            "Epoch: [13]  [ 100/1164]  eta: 0:09:05  lr: 0.001000  loss: 0.5413 (0.5632)  loss_classifier: 0.1788 (0.1868)  loss_box_reg: 0.2984 (0.2856)  loss_objectness: 0.0233 (0.0296)  loss_rpn_box_reg: 0.0427 (0.0612)  time: 0.4063  data: 0.0032  max mem: 3277\n",
            "Epoch: [13]  [ 200/1164]  eta: 0:07:24  lr: 0.001000  loss: 0.5492 (0.5562)  loss_classifier: 0.1812 (0.1831)  loss_box_reg: 0.2861 (0.2845)  loss_objectness: 0.0275 (0.0291)  loss_rpn_box_reg: 0.0445 (0.0596)  time: 0.4094  data: 0.0032  max mem: 3277\n",
            "Epoch: [13]  [ 300/1164]  eta: 0:06:23  lr: 0.001000  loss: 0.5835 (0.5634)  loss_classifier: 0.1873 (0.1843)  loss_box_reg: 0.2755 (0.2837)  loss_objectness: 0.0273 (0.0300)  loss_rpn_box_reg: 0.0576 (0.0655)  time: 0.4086  data: 0.0033  max mem: 3277\n",
            "Epoch: [13]  [ 400/1164]  eta: 0:05:32  lr: 0.001000  loss: 0.5739 (0.5684)  loss_classifier: 0.1773 (0.1855)  loss_box_reg: 0.3063 (0.2845)  loss_objectness: 0.0267 (0.0310)  loss_rpn_box_reg: 0.0479 (0.0673)  time: 0.4095  data: 0.0033  max mem: 3277\n",
            "Epoch: [13]  [ 500/1164]  eta: 0:04:45  lr: 0.001000  loss: 0.5625 (0.5711)  loss_classifier: 0.1812 (0.1863)  loss_box_reg: 0.2806 (0.2847)  loss_objectness: 0.0335 (0.0315)  loss_rpn_box_reg: 0.0444 (0.0686)  time: 0.4082  data: 0.0032  max mem: 3277\n",
            "Epoch: [13]  [ 600/1164]  eta: 0:04:00  lr: 0.001000  loss: 0.5435 (0.5684)  loss_classifier: 0.1796 (0.1863)  loss_box_reg: 0.2939 (0.2837)  loss_objectness: 0.0256 (0.0310)  loss_rpn_box_reg: 0.0322 (0.0674)  time: 0.4081  data: 0.0031  max mem: 3277\n",
            "Epoch: [13]  [ 700/1164]  eta: 0:03:16  lr: 0.001000  loss: 0.5706 (0.5703)  loss_classifier: 0.1841 (0.1873)  loss_box_reg: 0.2825 (0.2841)  loss_objectness: 0.0340 (0.0309)  loss_rpn_box_reg: 0.0597 (0.0680)  time: 0.4103  data: 0.0031  max mem: 3277\n",
            "Epoch: [13]  [ 800/1164]  eta: 0:02:33  lr: 0.001000  loss: 0.5777 (0.5688)  loss_classifier: 0.1750 (0.1862)  loss_box_reg: 0.2942 (0.2827)  loss_objectness: 0.0281 (0.0311)  loss_rpn_box_reg: 0.0531 (0.0687)  time: 0.4096  data: 0.0031  max mem: 3277\n",
            "Epoch: [13]  [ 900/1164]  eta: 0:01:51  lr: 0.001000  loss: 0.5403 (0.5678)  loss_classifier: 0.1710 (0.1857)  loss_box_reg: 0.2734 (0.2821)  loss_objectness: 0.0271 (0.0314)  loss_rpn_box_reg: 0.0538 (0.0686)  time: 0.4091  data: 0.0031  max mem: 3277\n",
            "Epoch: [13]  [1000/1164]  eta: 0:01:08  lr: 0.001000  loss: 0.5507 (0.5656)  loss_classifier: 0.1692 (0.1849)  loss_box_reg: 0.2766 (0.2818)  loss_objectness: 0.0237 (0.0314)  loss_rpn_box_reg: 0.0416 (0.0675)  time: 0.4098  data: 0.0031  max mem: 3277\n",
            "Epoch: [13]  [1100/1164]  eta: 0:00:26  lr: 0.001000  loss: 0.5039 (0.5651)  loss_classifier: 0.1630 (0.1849)  loss_box_reg: 0.2675 (0.2813)  loss_objectness: 0.0249 (0.0313)  loss_rpn_box_reg: 0.0365 (0.0675)  time: 0.4079  data: 0.0032  max mem: 3277\n"
          ]
        }
      ],
      "source": [
        "# Training model on our data\n",
        "! python ../fastercnn-pytorch-training-pipeline/train.py --model fasterrcnn_resnet50_fpn --epochs 40 --data ../fastercnn-pytorch-training-pipeline/data_configs/plantdoc.yaml --use-train-aug --name fasterrcnn_resnet50_fpn_v2_trainaug_30e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input': '../input/inference_data/images/', 'data': None, 'model': None, 'weights': 'outputs/training/fasterrcnn_resnet50_fpn_v2_trainaug_30e/best_model.pth', 'threshold': 0.8, 'show': False, 'mpl_show': False, 'device': device(type='cuda', index=0), 'imgsz': 640, 'no_labels': False, 'square_img': False} -----------------------\n",
            "Building from model name arguments...\n",
            "Test instances: 8\n",
            "Image 1 done...\n",
            "--------------------------------------------------\n",
            "Image 2 done...\n",
            "--------------------------------------------------\n",
            "Image 3 done...\n",
            "--------------------------------------------------\n",
            "Image 4 done...\n",
            "--------------------------------------------------\n",
            "Image 5 done...\n",
            "--------------------------------------------------\n",
            "Image 6 done...\n",
            "--------------------------------------------------\n",
            "Image 7 done...\n",
            "--------------------------------------------------\n",
            "Image 8 done...\n",
            "--------------------------------------------------\n",
            "TEST PREDICTIONS COMPLETE\n",
            "Average FPS: 12.716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "! python ../fastercnn-pytorch-training-pipeline/inference.py --weights outputs/training/fasterrcnn_resnet50_fpn_v2_trainaug_30e/best_model.pth --input ../input/inference_data/images/ --threshold 0.8 --imgsz 640"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lczuLKgpGuBx"
      },
      "source": [
        "### 3. Выбор фреймворка/библиотеки для разработки веб/мобильного демо (0 баллов)\n",
        "\n",
        "Основным инструментом для разработки веб-демо будет микрофреймворк **Flask**: [серия туториалов на русском](https://habr.com/ru/post/346306/).\n",
        "Полезные ресуры:\n",
        "- [курс по веб-разработке](https://www.youtube.com/playlist?list=PLzQrZe3EemP5KsgWGnmC0QrOzQqjg3Kd5), нас интересуют первые 7 видео в плейлисте. В частности, нужны видео по Flask, там очень хорошие обучалки параллельно с лектором;\n",
        "- [исчерпывающий справочник по Flask (англ)](https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python3_Flask.html);\n",
        "- можно посмотреть мой [репозиторий с реализацией веб-демо](https://github.com/izaharkin/Respalyzer) для ML-задачи оценки отзывов.\n",
        "\n",
        "Для разработки мобильного демо стоит выбрать инстурмент на свое усмотрение:\n",
        "- под Android: [пример на Pytorch Mobile](https://towardsdatascience.com/object-detector-android-app-using-pytorch-mobile-neural-network-407c419b56cd), [пример на TensorFlow Lite](https://www.tensorflow.org/lite/models/object_detection/overview). **Примечание** от Дмитрия Шумилина: на Android с TF Lite на момент января 2021 есть [ошибка](https://github.com/tensorflow/models/issues/9341) с новым форматом хранения модели. Можно попробовать возможное [решение](https://www.youtube.com/watch?v=syTKGY-H44E&ab_channel=DoomsdayRobotics) или писать на чистом Java. Также можно попробовать использовать более старые версии TensorFlow, в которых проблем совместимости еще не было, например, [v2.1.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0).\n",
        "- под iOS: [пример на TensorFlow Lite Swift API](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift), [пример с Vision Framework](https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture) на \"чистом\" Swift'е.\n",
        "\n",
        "Разумеется, лучше **самостоятельно поискать видео/статьи** на тему использования моделей на мобильных устройствах."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fpGAvez5GuBx"
      },
      "source": [
        "> Результатом пункта является зафисированный для вас инструмент для разработки демо."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oP_hQjnF-Zrx"
      },
      "source": [
        "#### Пользовательский интерфейс будет реализован через телеграмм бот"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ggyC5X6nGuBy"
      },
      "source": [
        "### 4. Разработка демо (3 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zqsnd7mkGuBy"
      },
      "source": [
        "Этот пункт про сам процесс написания кода для демо.\n",
        "\n",
        "> Результатом пункта является код, который можно запустить. Не хватать будет только логики детектора, сам интерфейс должен быть уже рабочим."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvdSnglS-ah1"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_HbHX0oAGuBz"
      },
      "source": [
        "### 5. Встраивание модели-детектора в демо (2 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bUgXk1UBGuB0"
      },
      "source": [
        "Этот пункт про процесс дописывания кода, который будет обеспечивать \"логику\" демо $-$ само детектирование.\n",
        "\n",
        "> Результатом пункта является код, который можно запустить и продемонстрировать работающую систему детектирования объектов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtuhzSzc-bav"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hC57dH9sGuB0"
      },
      "source": [
        "### 6. Тестирование демо (1 балл)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BbuoADSeGuB1"
      },
      "source": [
        "Здесь нужно запустить ваше демо на как можно большем количестве примеров, чтобы понять, в чем его сильные и слабые стороны. То есть какие объекты/сцены детектор обрабатывает легко, а с какими ему справится сложно. Нужно предложить также пути для улучшения модели на основе увиденных ошибок."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3-_tW90YGuB1"
      },
      "source": [
        "> Результатом пункта является набор изображений, на которых демо отработало. Для каждого изображения нужно добавить комментарии, почему модель справилась хорошо/плохо, предложить пути ее улучшения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz7kfX0J-cJq"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_AQ1MMGuB2"
      },
      "source": [
        "### 7. Улучшение дизайна / Развертывание демо на сервере (1 балл)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTEuUxKGuB3"
      },
      "source": [
        "В этом пункте можно пойти двумя путями:\n",
        "1. Проделать работу по улучшению визуальной составляющей демо (интерфейс)\n",
        "2. Загрузить модель на какой-нибудь сервер/хост/test-flight (в случае мобильного iOS-демо), чтобы к демо можно было обратиться прямо в адресной строке браузера / найти в Telegram\n",
        "\n",
        "\\> По *первому пункту* могу посоветовать использовать библиотеку [Bootstrap](https://habr.com/ru/post/349060/), для мобильного демо элементы UI/UX являются частью основной разработки (поэтому стоит просто погуглить/почитать документацию).\n",
        "\n",
        "\\> *Второй пункт - в случае веб-демо*:\n",
        "\n",
        "Способ 1: Google Cloud Engine.\n",
        "\n",
        "Если ваше приложение требует установки системных пакетов, например, через `apt-get install`, то вам придется работать на выделенном сервере VPN или на виртуальной машине. К счастью тот же [Google Cloud](https://cloud.google.com/compute) предоставляет бесплатные 300$ на 90 дней использования Виртуальной машиной, чего хватит в большинстве случаев. Эти ссылки помогут вам понять, как в таком случае создать виртуальную машину, установить и настроить виртуальное окружение и вебсервер, а также задеплоить проект:\n",
        "\n",
        "- [Deploying a Flask app to a Virtual Machine](https://www.youtube.com/watch?v=a2g9pDleGQk&ab_channel=JulianNash)\n",
        "- [Set up Gunicorn and Nginx](https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-20-04-ru)\n",
        "\n",
        "Способ 2: Heroku.\n",
        "\n",
        "Если с GCE проблемы/не хочется привязывать карту и т.д., могут помочь эти ресурсы и сервис [Heroku](https://www.heroku.com/):\n",
        "- [Flask deployment](http://www.tutorialspoint.com/flask/flask_deployment.htm)\n",
        "- [Deploy Flask app to Heroku (youtube)](https://www.youtube.com/watch?v=pmRT8QQLIqk)\n",
        "- [Deploy Flask app to Heroku (medium)](https://medium.com/the-andela-way/deploying-your-flask-application-to-heroku-c99050bce8f9)\n",
        "- [Set your own domain name on Heroku](https://devcenter.heroku.com/articles/custom-domains)\n",
        "\n",
        "\\> *Второй пункт - в случае мобильного демо*:\n",
        "\n",
        "Здесь с как таковым деплоем сложнее, обычно мобильные приложения публикуются или в Google Play (Android), или в AppStore (iOS). Однако можно снять **видеопоказ экрана (скринкаст)** с использованием написанного приложения -- вполне подойдет для публичной демонстрации.\n",
        "\n",
        "> Результат пункта --- видео с описанием продукта и демонстрацией работы сервиса, который развернут в интернете и доступен для использования. Также необходимо кинуть ссылку на сам сервис, если удалось его развернуть."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggcKq6uf-eca"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G50tgxC9GuB-"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><b>Критерии оценивания</b></h2>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7FI0JdGuB-"
      },
      "source": [
        "* 1 пункт $-$ 1 балл\n",
        "* 2 пункт $-$ 1 балл\n",
        "* 3 пункт $-$ 0 баллов (промежуточный пункт)\n",
        "* 4 пункт $-$ 3 балла\n",
        "* 5 пункт $-$ 3 балла\n",
        "* 6 пункт $-$ 1 балл\n",
        "* 7 пункт $-$ 1 балл\n",
        "* Максимум баллов по проекту $-$ 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JZxROjYgGuB_"
      },
      "source": [
        "**Успехов в выполнении проекта!**\n",
        "\n",
        "Желаю всем проделать полезную, интересную и качественную работу, которую потом нестыдно и в резюме указать, и друзьям показать ;)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
