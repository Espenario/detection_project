{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dV66G0WGGuBj"
      },
      "source": [
        "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=500></p>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9LuFLCm3GuBl"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z7pPmypAGuBm"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Object detection</b></h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QLt9nwPoGuBn"
      },
      "source": [
        "### Руководитель проекта:\n",
        "* Юрий Яровиков (AIRI, МФТИ) | tg:@yu_rovikov"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IwjXeeaDGuBp"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Треки на проекте</b></h1>\n",
        "На этом проекте есть два возможных трека, из которых нужно выбрать один.\n",
        "\n",
        "* **Первый трек --- исследовательский**. На этом треке вам предстоит самостоятельно обучить и протестировать предобученную модель детекции. Основной упор делается на моделирование и обучение. Необходимо будет попробовать несколько моделей детекции, самостоятельно реализовать метрики.\n",
        "\n",
        "* **Второй трек --- продуктовый**. На этом треке вам не понадобится обучать свою модель детекции (хотя никто не запрещает вам это делать), но необходимо, во-первых, продумать **продуктовую составляющую проекта** (проблема людей, которая решается в данном проекте, целевая аудитория продукта, оптимальный способ внедрения модели), а также создать [MVP](https://ru.wikipedia.org/wiki/%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE_%D0%B6%D0%B8%D0%B7%D0%BD%D0%B5%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B4%D1%83%D0%BA%D1%82) , **внедрив модель в цифровой сервис**, который может быть реализован как Telegram-бот, Web-демо, Desktop-приложение.\n",
        "\n",
        "Вам необходимо выбрать основной сценарий, по которому вы пойдете, указав это при сдаче работы. При этом, никто не мешает вам совместить два трека, проведя и моделирование, и встраивание в демо. В этом случае мы рекомендуем пойти по **плану из второго трека**, а за моделирование будут ставиться бонусные баллы.\n",
        "\n",
        "Обратите внимание, что суммарный балл по проекту не может превышать 10. Максимальный балл можно получить на любом из двух треков."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "58WM1dAoU5cV"
      },
      "source": [
        "# Исследовательский трек\n",
        "На этом треке вам предстоит самостоятельно обучить и протестировать предобученную модель детекции. Основной упор делается на моделирование и обучение. Необходимо попробовать несколько моделей детекции и провести их объективное сравнение в соответствии с целевой метрикой проекта.\n",
        "\n",
        "## План работы\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_JHEakt_Bb6"
      },
      "source": [
        "### 1. Выбор фреймворка/библиотеки для использования детектора (1 балл)\n",
        "\n",
        "Чтобы освежить память о задаче детекции, можно посмотреть [занятия на продвинутом курсе](https://stepik.org/lesson/458312/step/1?unit=616130).\n",
        "\n",
        "В выборе фреймворка предоставляется свобода, лично я рекомендовал бы один из:\n",
        "- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n",
        "- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n",
        "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n",
        "- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
        "\n",
        "**Обратите внимание, что для получения полного балла по проекту необходимо обучить и сравнить как минимум две различные модели детекции (можно из одного фреймворка)!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l3mg1D_W4g-"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OkobRpd6W3An"
      },
      "source": [
        "### 2. Запуск детектора на случайных изображениях (1 балл)\n",
        "\n",
        "В этом пункте вам необходимо применить модель детектирования в выбранном выше репозитории (по сути проверить, что инференс в модели работает). Таким образом, вы убедитесь, что модель работает, и сможет переходить к обучению.\n",
        "\n",
        "> Результатом пункта явлется набор изображений, на которых модель успешно отработала и результат детекции виден и понятен.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISt6_q_FW5Jm"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "90z-NuSbGuB5"
      },
      "source": [
        "### 3. Выбор датасета (0 баллов)\n",
        "\n",
        "Вы можете выбрать любой датасет для детекции. Вот несколько идей:\n",
        "1. [Детекция игровых карт](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10): лежат в папке images вместе с разметкой;\n",
        "2. [Детекция фруктов](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection): скачать можно, нажав на кнопку Download;\n",
        "3. [Детекция одежды (Deep Fashion 2)](https://github.com/switchablenorms/DeepFashion2): стоит прочитать README на главной странице репозитория. Для получения датасета нужно запросить пароль у автора через гугл-форму. После скачивания распакуйте его с использованием пароля. Из файлов аннотаций нас будут интересовать только `bounding_box`, `category_name` и `category_id`;\n",
        "4. [Детекция лиц (Wider Face)](http://shuoyang1213.me/WIDERFACE/): большой датасет для детектирования лиц самых разных размеров. Скачать можно прямо по ссылкам на сайте;\n",
        "5. [Детекция лиц (Kaggle)](https://www.kaggle.com/dataturks/face-detection-in-images): в датасете достаточно мало данных, но можно попробовать, если датасеты выше показались неподходящими для Вас;\n",
        "6. Датасет из любого соревновани по детекции на Kaggle.\n",
        "\n",
        "При работе с датасетом вы неизбежно столкнетесь с работой с файлами и папками (директориями). Рекомендуется освежить в памяти работу с библиотеками `os`, `json`, `glob`. Может помочь [этот туториал](https://realpython.com/working-with-files-in-python/).\n",
        "\n",
        "> Результатом выполнения пункта явлется загруженный датасет, состоящий из изображений и разметки к ним (bounding box'ов всех объектов на каждом изображении)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LEL6VxDXZcY"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Yz0JAWGuB6"
      },
      "source": [
        "### 4. Предобработка данных (2 балла)\n",
        "\n",
        "Самый непростой этап в этом сценарии. Скачать данные $-$ лишь половина дела. Чтобы обучить нейросеть на этих данных, нужно написать генератор батчей. Однако если будем подавать изображения так, как они есть, то даже батч собрать не сможем -- нужно привести их к однмоу размеру. Далее нужно привести их к типу float, переместить на CUDA и поделить значения в пикселях на 255 (подробнее см. [занятие](https://www.youtube.com/watch?v=XSPYe4-y4HE)). Также нужно настроить аугментации и постобработку.\n",
        "\n",
        "То, как именно все это реализовать $-$ зависит от инструмента, выбранного в пункте 1. Например, в detectron2 в обучающих материалах описан формат данных для обучения. Возможно, нужно будет зайти в документацию и почитать более подробно, чтобы разобраться, какой именно нужен формат координат.\n",
        "\n",
        "НЕ нужно копировать все файлы с картинками и разметкой прямо на диске в их предобработанные версии. Хороший тон $-$ осуществлять всю эту обработку программно, \"на лету\". Поможет [туториал](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) по написанию своего датасета на PyTorch.\n",
        "\n",
        "> Результатом выполнения пункта явлется код, запуск которого ведет к подаче батчей правильного вида (разметка приведена к требуемому формату координат, изображения нужного типа, размера и поделены на 255 и т.д.) для обучения нейронной сети-детектора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpDRtOU4XdP-"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3kOKLO9FGuB7"
      },
      "source": [
        "### 5. Обучение моделей-детекторов (3 балла)\n",
        "\n",
        "Необходимо написать цикл обучения на PyTorch самостоятельно -- это основной критерий в этом пункте. Необходимо обучить обе выбранные модели."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2XAXboBIGuB7"
      },
      "source": [
        "> Результатом выполнения пункта явлется код, запуск которого ведет к обучению модели на выбранном датасете. При обучении **обязательно выводить числовые значения лосса на трейне и валидации**, крайне желательно использовать [`TensorBoard`](https://pytorch.org/docs/stable/tensorboard.html) для визуализации. Обязательно также сохранять модель после каждой N-ой эпохи, чтобы потом ее качество можно было проверить и веса были переиспользуемыми."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LyZ2CgtoGuB8"
      },
      "source": [
        "### 6. Измерение качества работы модели (метрики согласуются с руководителем и зависят от задачи) (2 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "agZ_BOjEGuB9"
      },
      "source": [
        "Под метриками понимаются функции/формулы, по которым оценивается качество модели-детектора. Обычно для измерения качества работы детектора используют поклассовые Precision, Recall, F1-меру и mean Average Precision (mAP). Подробнее про них можно послушать в [видеолекции 2018 года](https://www.youtube.com/watch?v=ewkSI2cuyoQ&list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3&index=24).\n",
        "\n",
        "**Необходимо самостоятельно реализовать требуемые метрики!**\n",
        "\n",
        "> Результат пункта --- реализованные функции метрик для задачи детектирования, позволяющие оценить качество работы модели на выборке, а также оценка обеих обученных моделей по данным метрикам на test. Необходимо сделать вывод о том, какая модель сработала лучше и оценить полученный результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9IEmKseX3mw"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mnvIv-rbGuB9"
      },
      "source": [
        "### 7. Поиск путей применения этой модели в бизнесе/реальных задачах/набросок встраивания в веб/мобильное демо (1 балл)\n",
        "\n",
        "В этом пункте нужно подумать, как эта модель может быть использована в дальнейшем. То есть, например, зачем нужно детектировать фрукты? Или одежду?\n",
        "\n",
        "> Результат пункта $-$ перечисленные кейсы использования модели (описанные **как можно подробнее**).\n",
        "\n",
        "**IMPORTANT NOTE:** Обычно этим вопросом все же задаются до начала какой-либо разработки. Но поскольку проект носит учебный/исследовательский характер, допустимо говорить об этом в конце"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w5YtFZ0X3Gf"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTEStsXGuBv"
      },
      "source": [
        "# Продуктовый трек"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DbRJ7T6QGuBw"
      },
      "source": [
        "На этом треке вам не понадобится обучать свою модель детекции (хотя никто не запрещает вам это делать), но необходимо, во-первых, продумать **продуктовую составляющую проекта** (проблема людей, которая решается в данном проекте, целевая аудитория продукта, оптимальный способ внедрения модели), а также создать [MVP](https://ru.wikipedia.org/wiki/%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE_%D0%B6%D0%B8%D0%B7%D0%BD%D0%B5%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B4%D1%83%D0%BA%D1%82) , **внедрив модель детекции в цифровой сервис**, который может быть реализован как Telegram-бот, Web-демо, Desktop-приложение.\n",
        "\n",
        "Ваша модель не обязательно должна содержать в себе лишь детекцию: например, существуют составные модели, которые осуществляют детекцию лиц на фотографии и определяют их настроение/возраст. Такие модели тоже можно и даже желательно использовать, если того требует проект. Единственное требование --- чтобы детекция присутствовала в качестве основной/вспомогательной задачи.\n",
        "\n",
        "Если у Вас есть опыт веб- или мобильной разработки, можете работать в рамках привычных Вам инструментов. Главное, чтобы в итоге они позволяил встроить в себя нейросетевой детектор, на вход которому будут поступать картинки.\n",
        "\n",
        "Изображения на вход демо могут поступать с веб-камеры, из файлов, по ссылке или с камеры мобильного телефона -- способ должен вытекать из предполагаемого сценария применения вашего продукта. Демо должно показывать, что детектор успешно отрабатывает на поданных изображениях и находит нужные объекты."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vp4s0VHLZsOI"
      },
      "source": [
        "## План работы"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fTi_YuevZxqg"
      },
      "source": [
        "### 1. Поиск проблемы и описание решения (2 балла)\n",
        "В этом пункте необходимо сформулировать проблему реального мира и продумать, как именно она будет решаться с помощью вашего продукта.\n",
        "\n",
        "#### Как должен быть устроен ваш продукт\n",
        "Здесь мы не будем подробно обсуждать, как создавать IT-продукты, которые будут пользоваться широким спросом и способны генерировать выручку. Но кратко опишем!\n",
        "\n",
        "1. **Ваш продукт должен решать существующую проблему**. Исследования показывают, что это основная причина провала стартапов --- решение не существующей проблемы. О том, как создать стартап, который решает реальную проблему пользователя, можно прочитать [здесь](https://stfalcon.com/ru/blog/post/startups-solving-user-problems). Также есть отличная книжка \"Спроси маму\", которую необходимо прочитать любому человеку, который создает свой продукт.\n",
        "\n",
        "2. **Ваш продукт должен иметь целевую аудиторию**. Этот пункт увязан с предыдущим. Если у продукта нет целевой аудитории, его никто не будет использовать.\n",
        "\n",
        "3. **Ваш продукт должен быть оформлен в сервис, подходящий для основного сценария использования продукта и целевой аудитории**. Предположим, например, что вы делаете цифровой сервис для распознавания языка жестов. Как может выглядеть такой продукт и в какой сервис он может быть внедрён? Например, если создать ТГ-бота, который будет детектировать и распознавать жест по фотографии, его довольно сложно будет использовать, потому что каждый жест в отдельности сфотографировать нельзя. Оптимальным решением в этом случае было бы мобильное приложение с потоковым детектированием жеста на видео и автоматическим добавлением субтитров. При этом именно такой продукт может быть слишком сложен в реализации. Тогда необходимо выбрать оформление сервиса, которое будет осмысленно с продуктовой точки зрения и которое вы при этом сможете реализовать.\n",
        "\n",
        "#### Как искать проблему\n",
        "Есть много способов найти важную и актуальную проблему. Некоторые советы перечислены в книге \"Спроси маму\". Несколько коротких советов можно найти [здесь](https://vc.ru/life/1735-startup-ideas).\n",
        "* Можно подумать о темах, которые близки лично вам/вашим знакомым. Если проект решает проблему даже узкой целевой аудитории, это не страшно.\n",
        "* Можно найти уже существующий проект и улучшить его, обозначив, в чем преимущество вашего решения перед конкурентами.\n",
        "* У человечества вообще много глобальных [проблем](https://www.un.org/sustainabledevelopment/ru/sustainable-development-goals/), над которыми борются различные мировые организации. Если ваш проект способен хоть в каком-то частном случае продвинуться к решению этих проблем, это уже будет отлично.\n",
        "* Для поиска идей можно использовать датасеты с kaggle.\n",
        "\n",
        "> Результат пункта -- подробное описание проблемы, которую вы решаете, целевая аудитория использования продукта, а также **подробное** описание сервиса, который предлагается создать. Допускается описать \"идеальный продукт\", а затем создать MVP, имеющий отклонения от оптимального варианта, сославшись на ограниченное время/ресурсы для выполнения проекта. Но тогда это необходимо отдельно упомянуть в этом пункте."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hnfqSran-TZO"
      },
      "source": [
        "# Определение заболеваний растений\n",
        "## Проблема\n",
        "Все мы любим, когда на нашем загородном участке есть множество красивых цыетов и других различных растений. В то же время, не далеко не все из нас профессиональные агрономы, а красивых растений хочется всем. Значит, необходимо как-то научиться определять, чем болеет растение без вызова соответствующего специалиста.\n",
        "## Описание сервиса\n",
        "> **Телеграм-бот**\n",
        ">\n",
        "> Сервис планируется реализовать как телеграм-бот, в который можно будет загружать изображения и получать в качестве ответа фотографию с выделенными областями заражения растения и информацию о виде болезни.\n",
        ">\n",
        "> **Модели**\n",
        "> > *Простой вариант*\n",
        "> >\n",
        "> > Найти какой-нибудь готовый и размеченный датасет растений и применить на нем модель (будет использоваться 2-стадийный детектор, не очень конечно, но для mvp норм).\n",
        "> >\n",
        "> > *Интересный вариант*\n",
        "> >\n",
        "> > Подумать над аугментацией данных, добавлением различных растений, протестировать различные доступные модели (хотя бы singleshot). \n",
        ">\n",
        "> **ЦА**\n",
        ">\n",
        "> Дачкники-садоводы-любители\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nFRfmcVF4YrX"
      },
      "source": [
        "### 2. Поиск обученной модели и датасета (1 балл)\n",
        "\n",
        "В этом пункте вам необходимо выбрать модель, которую вы встроите в ваш продукт, и датасет, на котором вы эту модель будете тестировать.\n",
        "\n",
        "* Если вы найдете готовую модель, которую можно применить для вашей задачи, можно просто взять её. В этом случае с датасетом можно особо не заморачиваться. Достаточно в этом пункте запустить ваш детектор на нескольких (5-7) изображениях, на которых модель будет в итоге применяться, и проверить, что модель на них хорошо работает.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx3Gncop-VLr"
      },
      "source": [
        "Подходящей обученной модели в открытом доступе не нашлось, поэтому будем обучать сами, используя готовый шаблон и датасет\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGG31_2V7zo7"
      },
      "source": [
        "### Бонус. 2.5. Обучение модели для вашей задачи (5 баллов)\n",
        "**За этот пункт ставятся бонусные баллы. Он не является обязательным.**\n",
        "\n",
        "Если готовой обученной модели вы не смогли найти, тогда необходимо будет обучить модель самостоятельно. В таком случае перед выполнением пункта 2 вам необходимо будет найти подходящий датасет и обучить любую модель детекции с помощью встроенных методов из фреймворков, описанных в первом сценарии:\n",
        "- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n",
        "- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n",
        "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n",
        "- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
        "\n",
        "После обучения модель нужно будет протестировать на real-world изображениях, на которых планируется использовать продукт."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "2e2269748d836936f6becd38ff533c4bcfd9a31e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LDVp3cSEwpcQ",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using distributed mode\n",
            "device cuda\n",
            "Checking Labels and images...\n",
            "Checking Labels and images...\n",
            "Creating data loaders\n",
            "Number of training samples: 4656\n",
            "Number of validation samples: 478\n",
            "\n",
            "Building model from scratch...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: golovkin213171 (peppapig). Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.15.4\n",
            "wandb: Run data is saved locally in e:\\dls\\detection_project\\detection_stuff\\wandb\\run-20230703_210941-qah4f9ga\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run fasterrcnn_resnet50_fpn_v2_trainaug_30e\n",
            "wandb:  View project at https://wandb.ai/peppapig/detection_project\n",
            "wandb:  View run at https://wandb.ai/peppapig/detection_project/runs/qah4f9ga\n",
            "\n",
            "  0%|          | 0/4656 [00:00<?, ?it/s]\n",
            "100%|██████████| 4656/4656 [00:00<00:00, 112470.37it/s]\n",
            "\n",
            "  0%|          | 0/478 [00:00<?, ?it/s]\n",
            "100%|██████████| 478/478 [00:00<00:00, 477920.69it/s]\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to C:\\Users\\Pig_Astronaut/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
            "\n",
            "  0%|          | 0.00/167M [00:00<?, ?B/s]\n",
            "  0%|          | 416k/167M [00:00<00:41, 4.24MB/s]\n",
            "  2%|▏         | 3.53M/167M [00:00<00:08, 20.8MB/s]\n",
            "  5%|▍         | 8.25M/167M [00:00<00:04, 33.8MB/s]\n",
            "  8%|▊         | 13.1M/167M [00:00<00:03, 40.5MB/s]\n",
            " 11%|█         | 18.1M/167M [00:00<00:03, 44.4MB/s]\n",
            " 14%|█▎        | 22.9M/167M [00:00<00:03, 46.5MB/s]\n",
            " 17%|█▋        | 27.8M/167M [00:00<00:03, 47.8MB/s]\n",
            " 20%|█▉        | 32.6M/167M [00:00<00:02, 48.5MB/s]\n",
            " 22%|██▏       | 37.4M/167M [00:00<00:02, 48.8MB/s]\n",
            " 25%|██▌       | 42.3M/167M [00:01<00:02, 49.5MB/s]\n",
            " 28%|██▊       | 47.1M/167M [00:01<00:02, 48.9MB/s]\n",
            " 31%|███       | 51.9M/167M [00:01<00:02, 49.5MB/s]\n",
            " 34%|███▍      | 57.0M/167M [00:01<00:02, 50.6MB/s]\n",
            " 37%|███▋      | 61.8M/167M [00:01<00:02, 42.1MB/s]\n",
            " 42%|████▏     | 69.6M/167M [00:01<00:01, 52.4MB/s]\n",
            " 45%|████▍     | 74.9M/167M [00:01<00:01, 52.6MB/s]\n",
            " 48%|████▊     | 80.1M/167M [00:01<00:02, 44.5MB/s]\n",
            " 51%|█████     | 85.0M/167M [00:01<00:01, 46.2MB/s]\n",
            " 54%|█████▎    | 89.7M/167M [00:02<00:02, 36.1MB/s]\n",
            " 56%|█████▋    | 94.4M/167M [00:02<00:01, 38.8MB/s]\n",
            " 59%|█████▉    | 98.5M/167M [00:02<00:02, 32.9MB/s]\n",
            " 62%|██████▏   | 104M/167M [00:02<00:01, 37.9MB/s] \n",
            " 65%|██████▍   | 108M/167M [00:02<00:01, 31.6MB/s]\n",
            " 67%|██████▋   | 111M/167M [00:02<00:01, 32.6MB/s]\n",
            " 69%|██████▊   | 115M/167M [00:02<00:01, 33.2MB/s]\n",
            " 73%|███████▎  | 122M/167M [00:03<00:01, 43.4MB/s]\n",
            " 76%|███████▌  | 127M/167M [00:03<00:00, 45.2MB/s]\n",
            " 79%|███████▊  | 131M/167M [00:03<00:01, 31.7MB/s]\n",
            " 81%|████████  | 135M/167M [00:03<00:01, 32.4MB/s]\n",
            " 88%|████████▊ | 147M/167M [00:03<00:00, 46.1MB/s]\n",
            " 93%|█████████▎| 156M/167M [00:03<00:00, 55.9MB/s]\n",
            " 97%|█████████▋| 162M/167M [00:03<00:00, 50.0MB/s]\n",
            "100%|██████████| 167M/167M [00:04<00:00, 43.1MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"e:\\dls\\detection_project\\fastercnn-pytorch-training-pipeline\\train.py\", line 561, in <module>\n",
            "    main(args)\n",
            "  File \"e:\\dls\\detection_project\\fastercnn-pytorch-training-pipeline\\train.py\", line 347, in main\n",
            "    model = model.to(DEVICE)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "  File \"c:\\Users\\Pig_Astronaut\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 239, in _lazy_init\n",
            "    raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
            "AssertionError: Torch not compiled with CUDA enabled\n",
            "wandb: Waiting for W&B process to finish... (failed 1). Press Ctrl-C to abort syncing.\n",
            "wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
            "wandb: \\ 0.002 MB of 0.025 MB uploaded (0.000 MB deduped)\n",
            "wandb: | 0.025 MB of 0.025 MB uploaded (0.000 MB deduped)\n",
            "wandb:  View run fasterrcnn_resnet50_fpn_v2_trainaug_30e at: https://wandb.ai/peppapig/detection_project/runs/qah4f9ga\n",
            "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "wandb: Find logs at: .\\wandb\\run-20230703_210941-qah4f9ga\\logs\n"
          ]
        }
      ],
      "source": [
        "! python ../fastercnn-pytorch-training-pipeline/train.py --model fasterrcnn_resnet50_fpn_v2 --epochs 30 --data ../fastercnn-pytorch-training-pipeline/data_configs/plantdoc.yaml --use-train-aug --name fasterrcnn_resnet50_fpn_v2_trainaug_30e"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lczuLKgpGuBx"
      },
      "source": [
        "### 3. Выбор фреймворка/библиотеки для разработки веб/мобильного демо (0 баллов)\n",
        "\n",
        "Основным инструментом для разработки веб-демо будет микрофреймворк **Flask**: [серия туториалов на русском](https://habr.com/ru/post/346306/).\n",
        "Полезные ресуры:\n",
        "- [курс по веб-разработке](https://www.youtube.com/playlist?list=PLzQrZe3EemP5KsgWGnmC0QrOzQqjg3Kd5), нас интересуют первые 7 видео в плейлисте. В частности, нужны видео по Flask, там очень хорошие обучалки параллельно с лектором;\n",
        "- [исчерпывающий справочник по Flask (англ)](https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python3_Flask.html);\n",
        "- можно посмотреть мой [репозиторий с реализацией веб-демо](https://github.com/izaharkin/Respalyzer) для ML-задачи оценки отзывов.\n",
        "\n",
        "Для разработки мобильного демо стоит выбрать инстурмент на свое усмотрение:\n",
        "- под Android: [пример на Pytorch Mobile](https://towardsdatascience.com/object-detector-android-app-using-pytorch-mobile-neural-network-407c419b56cd), [пример на TensorFlow Lite](https://www.tensorflow.org/lite/models/object_detection/overview). **Примечание** от Дмитрия Шумилина: на Android с TF Lite на момент января 2021 есть [ошибка](https://github.com/tensorflow/models/issues/9341) с новым форматом хранения модели. Можно попробовать возможное [решение](https://www.youtube.com/watch?v=syTKGY-H44E&ab_channel=DoomsdayRobotics) или писать на чистом Java. Также можно попробовать использовать более старые версии TensorFlow, в которых проблем совместимости еще не было, например, [v2.1.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0).\n",
        "- под iOS: [пример на TensorFlow Lite Swift API](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift), [пример с Vision Framework](https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture) на \"чистом\" Swift'е.\n",
        "\n",
        "Разумеется, лучше **самостоятельно поискать видео/статьи** на тему использования моделей на мобильных устройствах."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fpGAvez5GuBx"
      },
      "source": [
        "> Результатом пункта является зафисированный для вас инструмент для разработки демо."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oP_hQjnF-Zrx"
      },
      "source": [
        "#### Пользовательский интерфейс будет реализован через телеграмм бот"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ggyC5X6nGuBy"
      },
      "source": [
        "### 4. Разработка демо (3 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zqsnd7mkGuBy"
      },
      "source": [
        "Этот пункт про сам процесс написания кода для демо.\n",
        "\n",
        "> Результатом пункта является код, который можно запустить. Не хватать будет только логики детектора, сам интерфейс должен быть уже рабочим."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvdSnglS-ah1"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_HbHX0oAGuBz"
      },
      "source": [
        "### 5. Встраивание модели-детектора в демо (2 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bUgXk1UBGuB0"
      },
      "source": [
        "Этот пункт про процесс дописывания кода, который будет обеспечивать \"логику\" демо $-$ само детектирование.\n",
        "\n",
        "> Результатом пункта является код, который можно запустить и продемонстрировать работающую систему детектирования объектов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtuhzSzc-bav"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hC57dH9sGuB0"
      },
      "source": [
        "### 6. Тестирование демо (1 балл)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BbuoADSeGuB1"
      },
      "source": [
        "Здесь нужно запустить ваше демо на как можно большем количестве примеров, чтобы понять, в чем его сильные и слабые стороны. То есть какие объекты/сцены детектор обрабатывает легко, а с какими ему справится сложно. Нужно предложить также пути для улучшения модели на основе увиденных ошибок."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3-_tW90YGuB1"
      },
      "source": [
        "> Результатом пункта является набор изображений, на которых демо отработало. Для каждого изображения нужно добавить комментарии, почему модель справилась хорошо/плохо, предложить пути ее улучшения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz7kfX0J-cJq"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_AQ1MMGuB2"
      },
      "source": [
        "### 7. Улучшение дизайна / Развертывание демо на сервере (1 балл)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTEuUxKGuB3"
      },
      "source": [
        "В этом пункте можно пойти двумя путями:\n",
        "1. Проделать работу по улучшению визуальной составляющей демо (интерфейс)\n",
        "2. Загрузить модель на какой-нибудь сервер/хост/test-flight (в случае мобильного iOS-демо), чтобы к демо можно было обратиться прямо в адресной строке браузера / найти в Telegram\n",
        "\n",
        "\\> По *первому пункту* могу посоветовать использовать библиотеку [Bootstrap](https://habr.com/ru/post/349060/), для мобильного демо элементы UI/UX являются частью основной разработки (поэтому стоит просто погуглить/почитать документацию).\n",
        "\n",
        "\\> *Второй пункт - в случае веб-демо*:\n",
        "\n",
        "Способ 1: Google Cloud Engine.\n",
        "\n",
        "Если ваше приложение требует установки системных пакетов, например, через `apt-get install`, то вам придется работать на выделенном сервере VPN или на виртуальной машине. К счастью тот же [Google Cloud](https://cloud.google.com/compute) предоставляет бесплатные 300$ на 90 дней использования Виртуальной машиной, чего хватит в большинстве случаев. Эти ссылки помогут вам понять, как в таком случае создать виртуальную машину, установить и настроить виртуальное окружение и вебсервер, а также задеплоить проект:\n",
        "\n",
        "- [Deploying a Flask app to a Virtual Machine](https://www.youtube.com/watch?v=a2g9pDleGQk&ab_channel=JulianNash)\n",
        "- [Set up Gunicorn and Nginx](https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-20-04-ru)\n",
        "\n",
        "Способ 2: Heroku.\n",
        "\n",
        "Если с GCE проблемы/не хочется привязывать карту и т.д., могут помочь эти ресурсы и сервис [Heroku](https://www.heroku.com/):\n",
        "- [Flask deployment](http://www.tutorialspoint.com/flask/flask_deployment.htm)\n",
        "- [Deploy Flask app to Heroku (youtube)](https://www.youtube.com/watch?v=pmRT8QQLIqk)\n",
        "- [Deploy Flask app to Heroku (medium)](https://medium.com/the-andela-way/deploying-your-flask-application-to-heroku-c99050bce8f9)\n",
        "- [Set your own domain name on Heroku](https://devcenter.heroku.com/articles/custom-domains)\n",
        "\n",
        "\\> *Второй пункт - в случае мобильного демо*:\n",
        "\n",
        "Здесь с как таковым деплоем сложнее, обычно мобильные приложения публикуются или в Google Play (Android), или в AppStore (iOS). Однако можно снять **видеопоказ экрана (скринкаст)** с использованием написанного приложения -- вполне подойдет для публичной демонстрации.\n",
        "\n",
        "> Результат пункта --- видео с описанием продукта и демонстрацией работы сервиса, который развернут в интернете и доступен для использования. Также необходимо кинуть ссылку на сам сервис, если удалось его развернуть."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggcKq6uf-eca"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G50tgxC9GuB-"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><b>Критерии оценивания</b></h2>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7FI0JdGuB-"
      },
      "source": [
        "* 1 пункт $-$ 1 балл\n",
        "* 2 пункт $-$ 1 балл\n",
        "* 3 пункт $-$ 0 баллов (промежуточный пункт)\n",
        "* 4 пункт $-$ 3 балла\n",
        "* 5 пункт $-$ 3 балла\n",
        "* 6 пункт $-$ 1 балл\n",
        "* 7 пункт $-$ 1 балл\n",
        "* Максимум баллов по проекту $-$ 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JZxROjYgGuB_"
      },
      "source": [
        "**Успехов в выполнении проекта!**\n",
        "\n",
        "Желаю всем проделать полезную, интересную и качественную работу, которую потом нестыдно и в резюме указать, и друзьям показать ;)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
